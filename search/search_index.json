{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Whether you are studying for CompTIA's Linux+ Exam, a class in college, or learning for work, you've come to the right place. These notes will follow the structure of the Linux+ Exam objectives but will be useful for anyone. </p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you have your own notes lying around of any subject you find, feel free to submit a pull request or leave an issue if you find any errors, even a typo.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#why-we-started-this","title":"Why We Started This","text":"<p>The idea is to find like minded individuals also studying the same certification you are, and all contribute to a shared repository of notes. After 2 months of studying for Linux+, I have many formatted notes which I host publicly on my website. While studying with a group on Discord, we have discussed the idea of hosting these on a public URL using Github Pages to host and MKdocs (Markdown Documentation) as a static site generator (SSG). The project is now public on Github and anyone who has written notes of a particular exam objective/section can create a pull request for there markdown file. Or if someone is studying and they see an error, they can request a change. Overall, this allows people to utilize others resources to study for free as well as collaborate with others to maintain motivation.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/","title":"1.1 Linux Fundumentals","text":""},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#file-system-hierarchy-fhs","title":"File System Hierarchy (FHS)","text":"<p>The FHS is a standard describing the layout of Unix-like systems. Regardless of what distribution you are on, you will always encounter these same directories, so it is worth understanding. Throughout exploring these directories you will come to learn that in Linux,  everying is a file. Your entire memory is represented in a single file as <code>/dev/mem</code>.  Each ongoing process is represented as a directory with associated files in <code>/proc</code>, and so on. Learning these directories and their role in the FHS is fundumental to Linux and give you foundation for the rest of the sections.</p> <p>Try listing (<code>ls</code>) each directory and explore on your own system.</p> <pre><code>ls /\n    bin   cdrom   dev  home  lib32  libx32    media  opt   root  sbin  swapfile\n    tmp  varboot   etc  lib   lib64  lost+found  mnt    proc  run   srv   sys   usr\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot","title":"/boot","text":"<p>Upon starting a Linux system, after the BIOS or EUFI firmware has selected a boot device, such as a hard disk or SSD, it searches for a bootloader within that device inside the /boot directory. Grub (Grand Unified Bootloader) then uncompresses the <code>vmlinuz</code> executable file into memory. <code>vmlinuz</code> is an image of the entire kernel. compressed into one file. </p> <p>The next step after the kernel has been loaded into memory, is Grub handing over control to the kernel to complete the boot process. A temporary root file system is created with <code>initrd</code> (initial ramdisk) that contains just enough loadable modules to give the kernel access to the rest of the hardware. </p> <p>In many systems, boot is a seperate partition, you can check this by listing block devices with <code>lsblk</code>. You can see on my NVME card, it is indeed a seperate partition.</p> <pre><code>lsblk\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nnvme0n1     259:0    0   1.8T  0 disk \n\u251c\u2500nvme0n1p1 259:1    0   260M  0 part /boot/efi\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#proc","title":"/proc","text":"<p>The process directory contains a subdirectory for every ongoing process, similar to what is displayed in Windows Task Manager or Linux <code>top</code> command. The process with PID (process ID) of 1, <code>/proc/1</code>, is a special process know as the init process which is responsible for starting essential system services, and which all other processes are children of. This is often managed by <code>systemd</code>, replacing <code>init</code>. </p> <p>proc is one of the three virtual/psuedo filesystems as it creates files dynamically, on the spot to represent the current processes. </p> <p>Besides ongoing processes, the <code>/proc</code> directory also contains some kernel and hardware information. At one point in history, this got messy and Linux developers decided to create another directory <code>/sys</code> to contain this.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#sys","title":"/sys","text":"<p>As the second virtual filesystem, sys generates files and directories to represent the current hardware layout. This provides an interface to the kernel.</p> <pre><code>ls sys\nblock  bus  class  dev  devices  firmware  fs  hypervisor  kernel  module  power\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#var","title":"/var","text":"<p><code>/var</code> contains variable data that is expected to change and grow during operation. This includes log files, mail, cache and more. <code>var/spool</code> contains files waiting to be excecuted such as printer ques and cron jobs. <code>/var/lib</code> contains state information of applications or the system, this is data that programs modify as they run, this is often for preserving condition of an application between instances. <code>/var/tmp</code> is for temporary files that persist across reboot. Unlike <code>/tmp</code> which is removed after a reboot.</p> <pre><code>ls var\nbackups  cache  lib  local  lock  log  mail opt  run  spool  tmp\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#usr","title":"/usr","text":"<p><code>/usr</code> contains user-related data, programs and resources. Files not necessary for single user mode, a minimal system state for troubleshooting and maintenance. Most command's binaries that are commonly ran in the terminal are within <code>/usr/bin</code>.  These are commands considered non-essential, as opposed to built in shell commands. </p> <pre><code>which cd\ncd is a shell builtin\n\nwhich ls\n/usr/bin/ls\n</code></pre> <p><code>/usr/local</code> contains locally installed software and files that are seperate from the core operating system and package management system.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#lib-bin-and-sbin","title":"/lib, /bin and /sbin","text":"<p><code>/lib</code> contains libraries for <code>/bin</code> and <code>/sbin</code>. sbin is essential binaries with superuser (root) priviledges. Bin is similar and contains trivial binaries such as <code>cat</code> and <code>ls</code> . Both are required for single user mode.  </p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#dev","title":"/dev","text":"<p><code>/dev</code> is the third virtual file system and contains a layout of devices. When you plugin a USB, it will appear as <code>/dev/sda</code>, if you plugin another, it will appear as <code>dev/sdb</code>. </p> <p>Most device files are either character or block type.  Using the long <code>-l</code> format of <code>ls</code>, we can see the type by the first letter. Character devices, which begin with a letter <code>c</code>, do not have buffering, and are accessed through a stream of sequential data, one byte after another. Block (<code>b</code>) devices are accessed through a cache, and get their name because they are often read/write to a block at a time.</p> <p>The first special character device, <code>dev/zero</code>,  provides an endless stream of null characters, which can be used to zero out a hard disk. <code>dev/null</code> is essentially a black hole that discards anything directed into it. <code>dev/urandom</code> is a unlimited cryptographic (psuedo) random number generator.</p> <pre><code>ls -l /dev\ncrw-rw-rw-   1 root root      1,     5 Oct 30 14:10 zero\ncrw-rw-rw-   1 root root      1,     3 Oct 30 14:10 null\ncrw-rw-rw-   1 root root      1,     9 Oct 30 14:10 urandom\n</code></pre> <p>Some use cases of <code>urandom</code> include generating passwords, and to generate entropy for TLS/SSL modules like <code>mod_ssl</code> on Apache web servers.</p> <pre><code>&lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c12;echo\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#etc","title":"/etc","text":"<p>At some point in Unix history, this likely stood for \"etcetera\", though now some like to squeeze in the acronym \"editable text configuration\". Although most simply pronounce it \"etsy\" and know it as the place for configuration files. </p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#opt","title":"/opt","text":"<p>The FHS standard defines <code>/opt</code> as \"software and add-on packages that are not part of the default installation\". Often when a company is deploying an application, they have the option of either placing it all in one directory in <code>/opt/[application]</code> or share it's files across <code>/usr/local/bin</code> and  <code>/usr/local/lib</code>. </p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot-process","title":"Boot Process","text":""},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#step-by-step-boot-process","title":"Step by Step Boot Process`","text":"<ol> <li>Firmware (UEFI/BIOS)<ul> <li>When you turn on a computer, the firmware built into the hardware initiates the boot process.</li> <li>It performs a power-on-self-test (POST) to check hardware components and initialize them.</li> <li>The firmware looks for a boot device, usually the hard drive or SSD, where the bootloader is stored.</li> </ul> </li> <li>Bootloader (GRUB)<ul> <li>Once the firmware identifies the boot device, it hands over control to the bootloader.</li> <li>The bootloader loads the kernel into memory and passes control to it.</li> </ul> </li> <li>Kernel<ul> <li>The kernel is the core of the operating system. It initializes hardware, manages memory, and provides essential services to user programs.</li> <li>It mounts the root file system, initializes drivers for hardware components, and starts user space initialization.</li> </ul> </li> <li>User Space<ul> <li>After the kernel initializes, it starts the user space by launching systemd (previously init). This is the first process and can be seen with <code>pstree</code> or <code>ps -p 1</code>.</li> </ul> </li> </ol> <p>Depending on the firmware (BIOS or EUFI), a hard disk or SSD will have 1 of 2 partitioning schemes, MBR or GPT.</p> <p>Master Boot Record (MRB) is used in older systems that is the first 512 bytes of a storage device which contains how and where the operating system is located in order to be booted. Legacy BIOS systems using MBR can only support drives up to 2TB and up to 4 partitions.</p> <p>The bootstrap binary of a BIOS equipped machine is located in the MBR of the first storage device.</p> <p>GUID Partition Table (GPT) is the newer standard that works with UEFI that can have any number of ESP (EFI System Partitions) anywhere an a drive to load multiple operating  systems, unlike MBR, which is limited to one.  GPT also supports virtually unlimited size and up to 128 partitions.</p> <p>EUFI boot process searches for EFI applications which are usually bootloaders, such as GRUB, which can either act as a EFI application or a BIOS bootloader for older setups. ESP is where EFI applications are stored and are automatically created on the storage device during OS installation.</p> <p>Older BIOS systems often overwrited MBRs during installation of multiple OS's, GPT makes this easier becuase ESP can be located anywhere on the drive, they are not location dependent, like MBR, which must be at the first 512 bytes of the drive.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot-files-and-commands","title":"Boot Files and Commands","text":"<p>mkinitrd - creates an initial ram disk image <code>initrd.img</code> which is used by the kernel for preloading the block device modules (such as IDE, SCSI or RAID) which are needed to access the root filesystem.  dracut -  a more modern alternative to <code>mkinitrd</code> grub-install - installs grub onto a device, usually <code>boot/grub</code> grub-mkconfig - either generates a new <code>grub.cfg</code> file or updates the existing one also located in <code>boot/grub</code> grub-update - stub for <code>grub-mkconfig\u00a0-o\u00a0/boot/grub/grub.cfg</code></p> <p>initrd.img -  initrd is mounted in memory to determine which kernel modules are are needed to support the current hardware. Explanation on forum vmlinuz - the entire compressed kernel which is uncompressed and begins the initial systemd/init process, which starts all other processes Both are used both during the initial OS install, and every boot therafter.</p> <p>Note: <code>vmlinuz</code> kernel image is loaded first by the bootloader, followed by the <code>initrd</code>. The <code>initrd</code> serves as an intermediary step to initialize hardware and load necessary drivers before transitioning to the actual root file system.</p> <p><code>initrd</code> and <code>initramfs</code> are just two different methods of loading a temporary root file system into memory to start early kernel modules. </p> <p>dracut similar to initrd in providing an initial ramdisk during boot process and is a more modern and flexible tool that dynamically generates ramdisk images based on the system's hardware. It has replaced initrd in some distributions.</p> <p>Here are two short descriptions of dracut from the man page and it's github repository repsectively: \"low-level tool for generating an initramfs/initrd image\" \"a new initramfs infrastructure\"</p> <p>Kernel Panic - Critical error condition that usually results in the entire system becoming unresponsive and requiring a reboot. Nothing is userspace runs during and after the system panics, it will instead immediately shutdown all other CPUs, and the current CPU processing the panic will never return to userspace</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot-sources","title":"Boot Sources","text":""},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#preboot-excecution-environments-pxe","title":"Preboot Excecution Environments (PXE)","text":"<p>PXE is meant for booting off of a network, as opposed to a local storage device. This is done through TFTP (Trivial File Transfer Protocol), a simpler FTP which does not require authentication,. Once connected, the server will tell the client where to locate the boot files. iPXE is the newer protocol which uses HTTP to pull files. PXE and iPXE are essentially just one approach to provisioning and automate the deployment of operating systems and configurations for servers.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#optical-disc-image-iso","title":"Optical Disc Image (ISO)","text":"<p>ISOs are copies of of entire optical disks such as DVDs or CDs. Although bootable USB drives have largely replaced optical media, ISO files can still be used to create bootable USB sticks. The most common tool to create bootable drives is Rufus, although on my Linux Mint system below, I have downloaded a Fedora ISO, which I can create into a bootable stick from the file manager itself.</p> <p></p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#basic-package-compilation-from-source","title":"Basic Package Compilation From Source","text":"<p>Although software is often installed with a package manager (APT, RPM, etc.), there are times when it is necessary to install from source code. This could be when you are writing the software yourself, or the software is not listed in your package manager's repositories. </p> <p>Linux distros rarely come with the compiling/build tools themselves. To install on Debian based distros, run <code>sudo apt install build-essential</code> or <code>sudo dnf groupinstall \"Development Tools\"</code> on Red Hat based distros. These both install a set of essential development tools and libraries necessary for building software from source code.</p> <p>The following three commands are commonly used together and in this order to build and install software from source code on Unix-like systems.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#configure","title":"./configure","text":"<p>Configures the software build process. It checks the system for dependencies, sets build options, and generates a Makefile from Makefile.in that contains instructions for compiling the software.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#make","title":"make","text":"<p>running <code>make</code> will use the Makefile to build the program. It compiles the source code and produces executable binaries. I recommend running <code>man make</code> and digging around, here is a snippet:</p> <pre><code>The  make  utility will determine automatically which pieces of a large program need to be \nrecompiled, and issue the commands to recompile them. ...  Our examples show C programs, since\nthey are  very  common,  but  you can use make with any programming language whose compiler \ncan be run with a shell command.  In fact, make is not limited to programs.  You can use it to\ndescribe  any task  where  some  files  must  be  updated  automatically from others whenever\nthe others change.\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#make-install","title":"make install","text":"<p>This command will also use the Makefile but will use it to install the program once it has been compiled. This involves installing binaries, libraries, and other necessary files to make the software available to use.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#example","title":"Example","text":"<p>To practice these commands, you can go to gnu.org and download the hello-2.12.tar.gz at the very bottom.</p> <p>You can extract the file with <code>tar -zxvf [file]</code>. The <code>z</code> flag is for g-zipped files, hence the <code>.gz</code>. <code>x</code> for extract, optional <code>v</code> for verbose output, and <code>f</code> to specify the file. This should create a directory of the extracted files.</p> <p>Once in that directory, <code>ls</code> to make note of the file <code>configure</code> , then run <code>./configure</code> to execute it. Now with a <code>makefile</code>, run <code>make</code>. This should create an executable <code>hello</code> which can be ran with <code>./hello</code>. You should get a \"Hello World!\" output on your terminal. But to install the program globally, run <code>sudo make install</code>, this will install the package's files in <code>/usr/local/bin</code>, <code>/usr/local/lib</code>, <code>/usr/local/man</code>, etc. </p> <pre><code>\u276f sudo make install\n[sudo] password for promptier:     \n./mkinstalldirs /usr/local/bin /usr/local/info\nmkdir /usr/local/info\n/usr/bin/install -c hello /usr/local/bin/hello\n/usr/bin/install -c -m 644 ./hello.info /usr/local/info/hello.info\n\u276f hello \nHello, world!\n\u276f which hello\n/usr/local/bin/hello\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#basic-storage-concepts","title":"Basic Storage Concepts","text":""},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#block-storage","title":"Block Storage","text":"<ul> <li>No Metadata, or very minimal (basic file attributes)</li> <li>Each block has a unique ID</li> <li>Filesystems are often built on top of blocks</li> <li>Accessed through iSCSI networking protocol, or traditional Fibre Channel which requires hardware (cables, adapters, switches, etc.). Read more here</li> <li>Used in SANs (Storage Area Network) or cloud based storage</li> <li>highly performant and scalable, though expensive</li> <li>Examples include relational databases, email servers and VMWare's Virtual Machine Filesystem (VMFS)</li> </ul>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#object-storage","title":"Object Storage","text":"<ul> <li>Relatively new concept, often used in the cloud, Amazon S3 is a common example.</li> <li>Often accessed through an API.</li> <li>stored in \"data lakes\" or \"data pools\" in a flat manner, with no file hierarchy.</li> <li>ease of searchability and cost efficient. Best for large unstructured data.</li> <li>each object has metadata (size, date modified, permissions, etc.) and an UUID.</li> <li>poor performance because of the heavy metadata overhead</li> </ul>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#file-storage","title":"File Storage","text":"<ul> <li>Oldest and most widely used storage system</li> <li>Used with NAS (Network Attached Storage)</li> <li>Tree-like, hierarchical structure with files within nested folders, each being a child or parent</li> <li>Not very scalable.</li> <li>Can be visualized with the <code>tree</code> command. </li> </ul> <pre><code>tree\n.\n\u251c\u2500\u2500 Desktop\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bash\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dep.txt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 folders.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tar\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 file1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 file2\n\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#filesystem-in-userspace-fuse","title":"Filesystem in Userspace (FUSE)","text":"<p>Used to mount and unmount filesystems in userspace without requiring root level, kernel access. Commonly used by <code>sshfs</code>, a tool to mount a remote filesystem using SFTP. Another example that uses FUSE are [[1.6 Package Management and Sandboxed Apps#Appimages|Appimages]]</p> <p>From the man page of <code>fusermount</code></p> <pre><code>Simple interface for userspace programs to export a virtual filesystem to the Linux kernel. \nIt also aims to provide a secure method for non privileged users to create and mount their \nnown filesystem implementations.\n</code></pre>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#raid","title":"RAID","text":"<p>RAID stand for Redundant Array of Independent/Inexpensive Disks. RAID gives a group of hard drives or block storage devices redundancy and data protection against disc failure. Different RAID levels offer different degrees of fault tolerance. Animated Video Explaining RAID 0, 1 and 10</p> <p>Raid 0 - Isn't even RAID in a sense, as it provides zero fault tolerance - Raid 0, or \"disk striping\", splits data across drives evenly. - With 4 drives and a 4 megabyte file being saved, 1 megabyte per drive is spread across and thus increases speed. This also means each drives space is being taken advantage of. - Very fast but if even one drive fails, all data is lost, this means RAID 0 isn't just not fault tolerant, it increases the chances of data loss.</p> <p>Raid 1 - Known as \"mirroring\", data is stored on a drive in addition to each mirror drive. Atleast 2 drives are required for RAID 2 -  A drive can fail and the controller will just use either use the first drive or any of it's mirrors for data recovery and continuous operation. - A disadvantage is less effective storage capacity as all data gets written at least twice</p> <p>Raid 5 - RAID 5 stripes data across each drive and provides fault tolerance with each drive having a parity. - Each parity is equivalent to an entire drive, meaning if you have 4 disks totaling 4 terabytes, only 3 terabytes will be used for actual data storage.  - Can only handle 1 disk failure at a time</p> <p>Raid 6 - A more fault tolerant RAID 5 that can handle 2 disk failures at once as parity is spread twice to all disks. - Minimum of 4 disks required. - In a setup of 4 disks totaling 4 terabytes, only 2 terabytes of actual data storage is available, as 2 disks are used to store a double parity. - Read performance is the same as RAID 5, but write performance is slower as it must write double the parity blocks instead of 1. Animated Video of RAID 5 and 6</p> <p>Raid 10 - 2 drives are mirrored in a RAID 1 setup, with a minimum of 4 drives. - Combines the fault tolerance of RAID 1 and the speed of RAID 0 - The downside is you can only use 50% of space for actual storage, the other half is mirrored.</p>"},{"location":"1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#listing-hardware-information","title":"Listing Hardware Information","text":"<p>Linux Kernel Modules related to hardware devices are also called drivers. <code>lspci</code> will list your PCI devices, each starting with a  unique address, which more info can be found with <code>lspci -s [address] -v OR -k</code>. This will show what kernel module is in use which can be found in <code>lsmod</code>.</p> <p>To find you network card, pipe the output of <code>lspci</code> into <code>grep -i</code> for an insensitive search.</p> <pre><code>lspci | grep -i network\n03:00.0 Network controller: MEDIATEK Corp. MT7921 802.11ax PCI Express Wireless Network Adapter\n</code></pre> <p>Now let's see what kernel modules the network card is using with the verbose <code>-v</code> output.</p> <pre><code>lspci -s 03:00.0 -v\n03:00.0 Network controller: MEDIATEK Corp. MT7921 802.11ax PCI Express Wireless Network Adapter\n    DeviceName: Realtek RTL8111E Ethernet LOM\n    Subsystem: Lenovo Device e0bc\n    Physical Slot: 0\n    Flags: bus master, fast devsel, latency 0, IRQ 73, IOMMU group 11\n    Memory at fc02000000 (64-bit, prefetchable) [size=1M]\n    Memory at fc02100000 (64-bit, prefetchable) [size=16K]\n    Memory at fc02104000 (64-bit, prefetchable) [size=4K]\n    Capabilities: &lt;access denied&gt;\n    Kernel driver in use: mt7921e\n    Kernel modules: mt7921e\n</code></pre> <p>We could then use <code>lsmod</code> to find more about the kernel module, such as the size and other modules that are dependent on it.</p> <pre><code>lsmod | grep mt7921e\nmt7921e                94208  0\n</code></pre> <p><code>lsusb</code> is similar to the previous command. With option -t, command lsusb shows the current USB device mappings as a hierarchical tree. A specific USB device can be found with the <code>-d</code> flag followed by the id.</p> <pre><code>lsusb -vd 1d6b:0003\n\n[very verbose output]\n</code></pre> <p><code>dmidecode</code> is a tool for dumping a computer's\u00a0DMI table contents in a human readable from. This includes detailed hardware information as well as serial numbers and BIOS revision that does not require probing the actual hardware. Use the type flag <code>-t</code> to specify what hardware, such as <code>dmidecode -t memory</code> or <code>dmidecode -t processor</code>.</p> <p>Here are some output from <code>dmidecode</code> from my computer.</p> <pre><code>BIOS Information\n        Vendor: LENOVO\n        Version: H3CN32WW(V2.02)\n        Release Date: 02/23/2022\n        BIOS Revision: 1.32\n        Firmware Revision: 1.32\n\nHandle 0x0001, DMI type 1, 27 bytes\nSystem Information\n        Manufacturer: LENOVO\n        Product Name: 82K2\n        Version: IdeaPad Gaming 3 15ACH6\n        Serial Number: MP2BK1DS\n        UUID: 1c45bdff-12cd-11ed-8c90-e4a8dfe66d9e\n\nHandle 0x0004, DMI type 4, 48 bytes\nProcessor Information\n        Socket Designation: FP6\n        Type: Central Processor\n        Family: Zen\n        Manufacturer: Advanced Micro Devices, Inc.\n</code></pre>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/","title":"1.2 Manage Files and Directories","text":""},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#file-editing","title":"File Editing","text":"<p>Note throughout this writing I use the words column and field interchangeably as well as record and row. </p> <p>In the Unix Philosophy, programs are written to handle text streams, as they are the universal interface. The following programs are essential for manipulating these text streams for common tasks at the terminal as well as more complex bash scripting in Linux. </p>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#cut","title":"Cut","text":"<p>The simplest command to manipulate fields is <code>cut</code>. The default delimiter is a tab but can be specified with <code>-d[character]</code> and only supports a single character. Include desired fields with <code>-f</code>.</p> <p>Let's take a look at the <code>/etc/passwd</code> file, which contains all human and non-human users on the system. </p> <pre><code>~$ cat /etc/passwd \nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\n</code></pre> <p>Now let's just grab the username and shell. I've set the delimiter <code>-d:</code> and specified I want fields 1-7 <code>-f1,7</code>.</p> <pre><code>~$ cut -d: -f1,7 /etc/passwd\nroot:/bin/bash\ndaemon:/usr/sbin/nologin\nbin:/usr/sbin/nologin\n</code></pre>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#awk","title":"Awk","text":"<p>The <code>awk</code> command is a much larger utility and is an entire programming language in and of itself. It supports both columns and rows. Awk is often compared to Perl (Practical Extracting and Reporting Language) for complex text/data processing jobs, such as handling large csv files.</p> <p>To specify your field's with the <code>-F</code> flag, and make sure it's uppercase. Let's print out just the username and shell of <code>etc/passwd</code> as before with the cut command.</p> <pre><code>awk -F: '{print $1, $7}' /etc/passwd\nroot /bin/bash\ndaemon /usr/sbin/nologin\nbin /usr/sbin/nologin\n</code></pre> <p>Unlike cut, which just supports field separation, awk can select records (rows) with <code>NR</code> (number of record).</p> <pre><code>~$ awk -F: 'NR==2 {print $1, $7}' /etc/passwd\ndaemon /usr/sbin/nologin\n</code></pre> <p>If a record has 8 fields, <code>$8</code> will select the last field, as the number of fields (NF) is eight. In this case <code>$NF</code> would be equivalent to <code>$8</code>. Knowing this, we can always select the last field of any given record with <code>$NF</code>.</p> <pre><code>~$ awk -F: '{print $NF}' /etc/passwd\n/bin/bash\n/usr/sbin/nologin\n/usr/sbin/nologin\n</code></pre> <p>Awk is a very powerful tool and this is just the tip of the iceberg. But I hope was a good introduction.</p>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#sed","title":"Sed","text":"<p>Say we have a file <code>todo.txt</code> and want to manipulate by records/rows. </p> <pre><code>~$ cat todo.txt \n1. water plants\n2. walk dog\n3. finish homework\n4. study linux\n</code></pre> <p>By default, sed will print out the line specified, then the whole file again. </p> <pre><code>~$ sed '1p' todo.txt \n1. water plants\n1. water plants\n2. walk dog\n3. finish homework\n4. study linux\n</code></pre> <p>To prevent this, use the <code>-n</code> flag for (n)o output. Think of it as (n)ot printing the whole file, rather only what you specify. </p> <pre><code>~$ sed -n '1p' todo.txt \n1. water plants\n</code></pre> <p>Specify ranges with a comma.</p> <pre><code>~$ sed -n '1,3p' todo.txt \n1. water plants\n2. walk dog\n3. finish homework\n</code></pre> <p>Use a semicolon as a delimiter.</p> <pre><code>~$ sed -n '1p;4p' todo.txt \n1. water plants\n4. study linux\n</code></pre> <p>When deleting lines, omit the <code>-n</code> flag.</p> <pre><code>~$ sed '1,2d' todo.txt \n3. finish homework\n4. study linux\n</code></pre> <p>Besides selecting records of a file to print, we can find and replace with regex-like syntax. Say I have a dockerfile and I've decided to change the username. Instead of manually editing the file by changing each instance, I can use sed to substitute each value.</p> <pre><code>~$ sed 's/promptier/NEWUSERNAME/' Dockerfile \nFROM ubuntu\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y sudo &amp;&amp; \\\n    useradd -m NEWUSERNAME &amp;&amp; \\\n    echo \"NEWUSERNAME:1234\" | chpasswd &amp;&amp; \\\n    adduser NEWUSERNAME sudo\n\nUSER NEWUSERNAME\n</code></pre> <p>As this example only prints to STDOUT, we can choose instead to edit the file in place with the <code>-i</code> flag or create a new file.</p> <pre><code>~$ sed -i 's/promptier/NEWUSERNAME/' Dockerfile \nOR\n~$ sed 's/promptier/NEWUSERNAME/' Dockerfile &gt; Dockerfile2\n</code></pre> <p>Keep in mind substituting will run a single time on each line. If there were two or more instances of \"promptier\" on single line, I would add the global flag <code>-g</code> at the end to replace all instances : <code>sed 's/promptier/NEWUSERNAME/g'</code></p> <p>Two useful characters to know are the caret <code>^</code> and dollar sign <code>$</code> which represent the beginning and end of a line. These are common regex syntax that are used in both sed and vim. As an example, the way you would delete all blank lines in vim would be to check if the beginning of the line is immediately proceeded by the end of the line.</p> <pre><code>~$ cat todo.txt \n1. water plants\n\n2. walk dog\n\n3. finish homework\n\n4. study linux\n\n~$ sed '/^$/d' todo.txt \n1. water plants\n2. walk dog\n3. finish homework\n4. study linux\n</code></pre> <p>It is possible to insert lines before and after a match with <code>i</code> and <code>a</code>. Here I add a note after \"walk dog\". I could optionally insert before the line with <code>i</code>. Note the insertion begins with a backslash.</p> <pre><code>\u276f sed '/dog/a\\  -remember a leash' todo.txt \n1. water plants\n2. walk dog\n  -remember a leash\n3. finish homework\n4. study linux\n</code></pre> <p>Swap characters with <code>y</code>. This is identical to the translate <code>tr</code> command.</p> <pre><code>\u276f sed 'y/sed/XYZ/' todo.txt \n1. watYr plantX\n2. walk Zog\n3. finiXh homYwork\n4. XtuZy linux\n\n\u276f cat todo.txt | tr 'sed' 'XYZ' \n1. watYr plantX\n2. walk Zog\n3. finiXh homYwork\n4. XtuZy linux\n</code></pre> <p>Say I have a bash script with many comments and newlines. I could make a short and concise version by removing the comments and removing newlines.</p> <pre><code>sed '/^#/d;/^$/d' verbose-script.sh &gt; short-script.sh\n</code></pre> <p>Say I have a very long file with authors and their relevant works and info, I could see on what lines \"Hubbard\" appears with <code>=</code>. This is very similar to <code>grep -n</code>.</p> <pre><code>\u276f sed -n '/Hubbard/=' poems.csv \n2\n3\n\u276f grep -n 'Hubbard' 18.csv \n2:1771/1798/E. H. (Elihu Hubbard)/Smith/8/SONNET I./Sent to Miss  , with a Braid of Hair.\n3:1771/1798/E. H. (Elihu Hubbard)/Smith/8/SONNET II./Sent to Mrs.  , with a Song.\n</code></pre>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#sort","title":"Sort","text":"<p><code>sort</code> command will sort alphabetically by the first column by default, and will separate columns by white space unless the separator is specified with  <code>-t</code>.</p> <p>Say I wanted to sort kernel modules by their size, I would numeric sort <code>-n</code> , then choose the second column/key with <code>-k 2</code>.</p> <pre><code>lsmod | sort -n -k 2\nModule                  Size  Used by\nalgif_hash             16384  1\nalgif_skcipher         16384  1\ncmac                   16384  2\n</code></pre> <p>Now say I want to sort the users in <code>/etc/passwd</code> based on their user ID in descending order, which is the third column seperated by colons. I can use <code>-t</code> for field seperator and <code>-k3</code> for the third column. For descending order I will use the reverse flag <code>-r</code> and make sure to numeric sort <code>-n</code>.  </p> <pre><code>sort -t: -rn -k3 /etc/passwd\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\njoseph:x:1000:1000:joseph,,,:/home/joseph:/bin/bash\nhplip:x:126:7:HPLIP system user,,,:/run/hplip:/bin/false\nfwupd-refresh:x:125:135:fwupd-refresh user,,,:/run/systemd:/usr/sbin/nologin\n</code></pre> <p>Keep in mind this is a file we are reading so it can come after the command, the previous command <code>lsmod</code> was not a file, so I had to pipe the output into <code>sort</code>.</p>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#file-metadata","title":"File Metadata","text":"<p>Inodes keep track of files and directories in Linux, they contain everything but the filename and contents. This includes permissions, timestamps, file size, and ownership. Find exact inodes with <code>ls -i</code>.</p> <pre><code>ls -i\n18219914 file1  18221167 file2  18256138 file3\n</code></pre> <p>Two commands are used to check metadata, <code>file</code> and <code>stat</code>. File is much simpler and is often used to check the type of a file. Stat shows much more information, including file size, modification and access time, user and group ID, etc. This is read directly from the Inode.</p> <pre><code>$ file obs.sh\nobs.sh: Bourne-Again shell script, ASCII text executable\n\n$ stat obs.sh\n  File: obs.sh\n  Size: 694         Blocks: 8          IO Block: 4096   regular file\nDevice: 802h/2050d  Inode: 18220124    Links: 2\nAccess: (0775/-rwxrwxr-x)  Uid: ( 1000/     joe)   Gid: ( 1000/     joe)\nAccess: 2023-12-09 12:58:05.139821191 -0600\nModify: 2023-10-15 11:13:26.272931896 -0500\nChange: 2023-12-09 12:58:02.799821244 -0600\n Birth: 2023-10-15 11:13:26.272931896 -0500\n</code></pre>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#archiving-and-compressing","title":"Archiving and Compressing","text":"<p>Compressed files use less disk space and download faster over a network. On Windows, often files are downloaded in a zip format, which then are unzipped (decompressed). When installing software on Linux, file are often archived and compressed with a <code>tar.gz</code> ending. An archive is simply a collection of files and/or directories with <code>tar</code> (tape archive). The <code>gz</code> ending means its been compressed using <code>gzip</code>. </p> <p>As an example, I will extract, meaning take from an archive, and decompress the files in <code>nvim-linux64.tar.gz</code>. <code>x</code> is extract, <code>v</code> for verbose, <code>f</code> for file, and <code>C</code> to place it in a directory. </p> <pre><code>sudo tar -xvf nvim-linux64.tar.gz -C /usr/local/lib\n</code></pre> <p>Note it is not necessary to mention the compression type, such as <code>z</code> for gzip, <code>J</code> for xz and so on. <code>tar</code> will automatically detect the compression type. These flags are only necessary when creating compressed archives.</p> <p>In all the compression tools, there is an direct relationship between CPU processing time (compression time) and compression ratio. What this means is there is a trade off. If you want the maximum amount of disk space saved, it will require more CPU processing power and time to compress. If you want to save on CPU processing or time, a lower compression ratio can be used, but will use more disk space.</p> <p>In general <code>xz</code> achieves the highest compression level, followed by <code>bzip2</code> and then <code>gzip</code>. In order to achieve better compression however <code>xz</code> usually takes the longest to complete, followed by <code>bzip2</code> and then <code>gzip</code>.</p> <p>Each program has similar syntax. With no arguments, they compress. <code>-d</code> will decompress. Each deletes the input file during compression or compression. Use <code>-k</code> to keep it, verbose <code>-v</code> to display compression ratio, and <code>-t</code> for integrity test on compressed files.   </p> <p>To demonstrate, I will create a 100 megabyte file, and compress using each. <code>dd</code> is a powerful but dangerous command, so be careful. <code>dev/zero</code> as the input file (<code>if</code>) is just an endless stream of null characters, which are ASCII code zero and signifies the end of a string (\\0).</p> <pre><code>\u276f dd if=/dev/zero of=bigfile bs=1M  count=100 \n100+0 records in\n100+0 records out\n104857600 bytes (105 MB, 100 MiB) copied, 0.0834263 s, 1.3 GB/s\n\u276f xz -k bigfile; gzip -k bigfile; bzip2 -k bigfile\n\u276f ls -lh\ntotal 101M\n-rw-rw-r-- 1 promptier promptier 100M Dec 23 10:11 bigfile\n-rw-rw-r-- 1 promptier promptier  113 Dec 23 10:11 bigfile.bz2\n-rw-rw-r-- 1 promptier promptier 100K Dec 23 10:11 bigfile.gz\n-rw-rw-r-- 1 promptier promptier  16K Dec 23 10:11 bigfile.xz\n</code></pre> <p>Comparing the sizes, gzip shrunk it down to 100K, while xz came in at 16K. bzip2 came at 113 bytes, which I am unsure how.</p>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#backup-with-dd","title":"Backup with dd","text":"<p><code>dd</code> is a tool that will copy everything from a file, bit by bit, in the same order. It is known by many names, including \"disk dump\", \"disk destroyer\" and \"(d)on't (d)o it\". It can be dangerous if you don't know what you are doing. Specify your input file <code>if</code> and output file <code>of</code>. The following are some common uses.</p> <p>Create backup image file of a drive then restore to another drive.</p> <pre><code>dd if=/dev/sda of=~/backup.img\ndd if=~/backup.img of=/dev/sdb\n</code></pre> <p>You could also copy directly to another drive if desired.</p> <p>We can also use special character devices to zero out/wipe a disk or add a layer of encryption.</p> <pre><code>dd if=/dev/zero of=/dev/sda\ndd if=/dev/urandom of=/dev/sdb\n</code></pre>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#text-editors","title":"Text Editors","text":"<p>Though I will not provide an entire guide of either Nano or Vim, I will touch on them briefly and give my advice on which you should spend your time using. </p> <p>Nano is easy to use and will perform the job just as well as Vim. However, if you plan to spend a lot of time at the terminal, learning <code>vim</code> will initially set you back in term of speed productivity, but will be worth the ROI in one to two weeks of use. </p> <p>Vim movements are not just confined to the editor. Try opening up a man page and you will find the same navigation keys such as <code>k</code> and <code>j</code> for up and down, <code>gg</code> and <code>G</code> to move to the top and bottom of the file, and <code>/</code> to search. </p> <p><code>less</code> is a program that displays the contents of a file one page/screen at a time, as opposed to using <code>cat</code> and then viewing the contents through stdout. Similar to man pages, less also has vim like navigation.  </p> <p>Overall, learning vim will not just help you while using the editor, but provide universal navigation across many programs on Unix-like systems. Additionally, once you have the Vim movements to muscle memory, you can bring these keybindings to any text editor or note taking app. Both Vscode and Obsidian are two examples which support vim, either nativity or through third party extension, this makes Vim a valuable skill to acquire for Linux and beyond. </p>"},{"location":"1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#rsync","title":"Rsync","text":"<p>Rsync (Remote Synchronization) was an algorithm presented in a PhD thesis for sorting and syncing for remote data update. In the first phase, a client and server is defined, the client initiates and connects to the server either locally, via remote shell or via network socket. After connections is established, the client and server assume the roles of sender and receiver respectively. Rsync Takes into account the file path, ownership, mode (read, write, etc.), permissions, size, modification time, and an optional checksum. Primarily, rsync determines which files differ between the sender and receiver by comparing the modification time and size of each file.</p> <p>The general use is to copy contents of <code>dir1</code> to <code>dir2</code>. Keep in mind if a file already exists in <code>dir2</code>, it will simply ignore it, unless there is a more recent modification time, in which case it will be updated, hence the nature of syncing.</p> <pre><code>~$ rsync -r dir1/ dir2\n</code></pre> <p>Instead of recursive <code>-r</code>, many use archive <code>-a</code>, which preserves links, timestamps, permission, etc. as well as recursively copies. Run <code>-v</code> for verbose. </p> <p>If copying a large number of files to a remote server, it is recommended to dry run first to simulate the command without actually running it to catch errors before they cause problems.</p> <pre><code>rsync -av --dry-run send-dir joe@8.8.8.212:/home/recieve-dir\n</code></pre> <p>Here was me backing up my downloads folder to an external USB.</p> <pre><code>sudo rsync -av /home/promptier/downloads/ /media/promptier/USB/downloads/\n</code></pre>"},{"location":"1.%200%20System%20Management/1.3%20Storage/","title":"1.3 Storage","text":"<p>Shawn Powers Links Archiving &amp; Compressing (tar, gzip, cpio) Copying Between Networks (scp, rsync, nc)</p>"},{"location":"1.%200%20System%20Management/1.3%20Storage/#systemd-and-fstab-file-system-mounting","title":"Systemd and fstab File System Mounting","text":"<p>The traditional way for auto-mounting filesystems is <code>/etc/fstab</code>. However, a more modern method with more flexibility is to create systemd unit files in <code>etc/systemd/system</code>. This allows more granular control over each mount point and allows dependency management between units, so you can control the order of mounting. It is also the preferred method for more complex and network-mounted filesystems.</p> <p>It is worth noting that systemd automatically generates <code>.mount</code> and <code>.automount</code> files from  <code>/etc/fstab</code> anyways. This simplifies the process of managing mounts, as you don't need to manually create systemd unit files for each mount point. Instead, you can continue to use the familiar <code>/etc/fstab</code> syntax and systemd handles the unit generation behind the scenes.</p> <p>Overall, <code>/etc/fstab</code> is an older and traditional way of configuring file system mounts in Linux, which systemd is still compatible with. Both methods are popular with <code>etc/fstab</code> preferred for simple mounts and manual systemd unit files preferred for more complex configurations.  </p> <p>Both are similar in displaying, what to mount, where to mount it, the type of filesystem, and any additional options.</p> <p><code>/etc/fstab</code></p> <pre><code># &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\nUUID=df155a8b-6b89 /               ext4    errors=remount-ro 0       1\nUUID=E89D-9509  /boot/efi       vfat    umask=0077      0       1\n/swapfile                                 none            swap\n</code></pre> <p><code>/etc/systemd/system/mnt-data.mount</code></p> <pre><code>[Unit]\nDescription=Data mount\n\n[Mount]\nWhat=/dev/disk/by-uuid/filesystem_UUID\nWhere=/mnt/data\nType=xfs\nOptions=defaults\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"1.%200%20System%20Management/1.3%20Storage/#luks-and-cryptsetup","title":"LUKS and Cryptsetup","text":"<p>Still working on this one.</p> <pre><code>sudo cryptsetup luksFormat /dev/sda\n[sudo] password for promptier:     \nWARNING: Device /dev/sda already contains a 'dos' partition signature.\n\nWARNING!\n========\nThis will overwrite data on /dev/sda irrevocably.\n\nAre you sure? (Type 'yes' in capital letters): \n</code></pre>"},{"location":"1.%200%20System%20Management/1.3%20Storage/#lvm","title":"LVM","text":"<p>3 Components 1. Physical Volumes (pvs) 2. Volume Groups (vgs) 3. Logical Volumes (lsv)</p> <p>2 have counterparts to traditional disk partitioning</p> Disk Partitioning System LVM Partitions Logical Volumes Disks Volume Groups <p>First you create physical volumes</p> <pre><code>\u276f sudo pvcreate /dev/sda1\n  Physical volume \"/dev/sda1\" successfully created.\n\u276f sudo pvs\n  PV         VG Fmt  Attr PSize   PFree  \n  /dev/sda1     lvm2 ---  &lt;28.91g &lt;28.91g\n</code></pre> <p>Now volume groups are collections of physical volumes.</p> <p>To add or remove physical volumes from logical volumes, use extend and reduce.</p> <pre><code>vgextend &lt;volume_group&gt; &lt;physical_volume1&gt; &lt;physical_volume2&gt; ....\n\nvgreduce &lt;volume_groupe&gt; &lt;physical_volume1&gt; &lt;physical_volume2&gt;\n</code></pre> <p>Finally, logical volumes are created from volume groups.</p> <pre><code>sudo lvcreate -L &lt;size&gt; -n &lt;lvname&gt; &lt;vgname&gt;\n</code></pre> <p>physical volumes &gt; volume groups &gt; logical volumes</p> <ol> <li>create physical volumes from multiple devices</li> </ol> <pre><code>sudo pvcreate /dev/sdc /dev/sdd\n</code></pre> <ol> <li>next create a volume group using multiple physical volumes</li> </ol> <pre><code>sudo vgcreate lvm_tutorial /dev/sdc /dev/sdd\n</code></pre> <ol> <li>finally create a logical volume from a volume group</li> </ol> <pre><code>sudo lvcreate -L 5GB -n my_lvm lvm_tutorial\n</code></pre> <p></p>"},{"location":"1.%200%20System%20Management/1.3%20Storage/#filesystems","title":"Filesystems","text":"<p>DJWare Benchmarks and Comparison video</p> <p>Problems that were solved at Facebook data-centers by switching to BTRFS in this article. BTRFS provided efficient container isolation, reducing IO and CPU expenses, and enabling advanced features like snapshotting and transparent file compression.</p> <p>Though the upstream Fedora Project uses BTRFS, REHL 9 uses XFS by default. This Reddit thread is very informative, which a Lead Engineer at Red Hat responds.</p> <p>Recent thread in Red Hat forum concerning filesystem usage in Red Hat and Suse.</p> <p>OpenSuse and SLES typically use Btrfs as the default file system for the root partition due to its advanced features like snapshots, rollbacks, and data deduplication. For other partitions, such as <code>/home</code> or data partitions, XFS is commonly recommended due to its stability and performance, especially when dealing with large files. This approach combines the advantages of both file systems: Btrfs for system file management and rollback capabilities, and XFS for high-performance data storage.</p> <p>Debian and Ubuntu use EXT4 as their default filesystem.</p>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/","title":"1.4 Processes and Services","text":""},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#systemctl","title":"Systemctl","text":"<p>The <code>systemctl</code> program inspects and manages the<code>systemd</code> system and service manager. Systemd runs as your your very first process (PID 1) on boot, which spawns all other processes. </p> <p><code>systemd status</code> will display all ongoing systemd services which can be narrowed down with <code>{service-name}</code>, such as the following.</p> <pre><code>~$ systemctl status cron\n\u25cf cron.service - Regular background program processing daemon\n     Loaded: loaded (/lib/systemd/system/cron.service; enabled; vendor preset: enabled)\n     Active: active (running) since Mon 2023-11-13 07:13:26 CST; 2h 31min ago\n</code></pre> <p>Each of these services are contained within control groups (cgroups) to isolates resource usage of a collection of processes. Very similar to <code>systemd status</code>, <code>systemd-cgls</code> will list all control groups in a more concise format. <code>systemd-cgtop</code> will list control groups by resource usage, similar to <code>top</code>.</p> <p>To manage these services, use <code>systemctl enable</code> and  <code>disable</code> for automatic starting or preventing the service at boot. Use <code>start</code> and <code>stop</code> to do this manually for immediate action. A more drastic action is <code>mask</code>, which prevents the service from starting on boot and manually starting by creating a symlink to <code>/dev/null</code>, this effectively discards any attempt to start the unit. As an example, if you have a legacy service (<code>old-service.service</code>) that should never be started again and should be completely removed from the system. Masking ensures that even if someone tries to start it manually, systemd will ignore it.</p>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#process-management","title":"Process Management","text":""},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#kill-signals","title":"Kill Signals","text":"<p>You can  list all possible kill signals with <code>kill -l</code>, here are four common ones with there name and associated code to be called with <code>kill</code>.</p> <p>SIGHUP - 1 Originating from the concept of a \"hangup\", or when a something like a modem disconnects from a console, this signal now mostly refers to a program losing connection to the controlling terminal (tty).  Any program started in a given terminal will receive a SIGHUP when the terminal is closed. Besides terminals, daemons (being detached from the terminal) use this signal by convention to reread config files.</p> <p>SIGINT - 2 Signal interrupt is what occurs when <code>ctrl + c</code> is pressed during a running process. It is essentially a interruption request sent by the user.</p> <p>SIGKILL - 9 Instructs the process to terminate immediately. It cannot be ignored or blocked. This is the most powerful signal and can have unintended consequences.   </p> <p>SIGTERM - 15 Signal terminate tries to kill the process, but can be blocked or handled in various ways. It is a more gentle way to kill a process.</p> <p>SIGCONT - 18 Continue a process after having been paused with SIGSTOP</p> <p>SIGSTOP - 19  This pauses a foreground process, which is one that controls the tty and is currently being displayed in the terminal. <code>ctrl + z</code> will trigger this signal and bring the process the the backround. The process can be brought back the the foreground with <code>fg</code>. </p> <p>Let's run a sleep command to start a running process in the foreground, pause it, find it's PID, then kill it. Keep in mind <code>kill -9</code> is equivelent to <code>kill -SIGKILL</code> and so forth.</p> <pre><code>\u276f sleep 200\n^Z\n[1]+  Stopped                 sleep 200\n\n~ took 3s \n\u2726 \u276f ps\n    PID TTY          TIME CMD\n   8305 pts/0    00:00:00 bash\n  11615 pts/0    00:00:00 sleep\n  11645 pts/0    00:00:00 ps\n\n~ \n\u2726 \u276f kill -9 11615\n</code></pre> <p>Alternatively, we could have induced <code>SIGINT</code> with <code>ctrl + c</code> to interupt the process, or brought back the backround process to our terminal with <code>fg</code>.</p> <p>Commands to find PIDs by name include <code>pgrep</code> and <code>pidof</code>. They are similar except pidof only returns the exact match of the name, while pgrep includes partial matches. </p> <p>In this example, pidof returns only the exact match called <code>systemd</code>, while pgrep returns programs including <code>systemd-journal</code>, <code>systemd-logind</code>, etc.</p> <pre><code>\u276f pidof systemd\n1779\n\n\u276f pgrep systemd\n1\n460\n495\n892\n1779\n</code></pre> <p>The the <code>kill</code> program requires the exact PID, there are two programs which take names as arguments. Comparable to pgrep and pidof, <code>pkill</code> and <code>killall</code> work with partial names and exact names respectively. In this example, pkill will forcibly kill all programs with matching \"obsidian\" including partial matches. killall will only kill programs with the exact name of \"obsidian\".</p> <pre><code>\u276f pkill -9 obsidian\n\u276f killall -9 obsidian\n</code></pre> <p>Here I pause (SIGSTOP) and resume (SIGCONT) my file manager, Nemo, on Linux Mint. After I pause it, the application's window is basically frozen, then I resume it. </p> <pre><code>\u276f pkill -19 nemo\n\n\u276f pkill -18 nemo\n</code></pre>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#process-states","title":"Process States","text":"Process State PS Representation Running R Idle I Stopped T Sleeping(Int) S Sleeping(UnInt) D Zombie Z <p>These process states can be seen with <code>ps</code> and <code>top</code>. Processes that are stopped can be though of as paused, this is what happens with <code>ctrl + c</code> sending the SIGSTOP signal, which can be resumed with SIGCONT. Most processes seen with <code>top</code> are either running (S), idle (I).</p> <p>Interruptible sleep (S) is sensitive and waiting for user space signals. Uninterruptible sleep (D) waits for a specific system call (usually disk I/O) and cannot be interrupted or killed until the call is complete.</p> <p>Zombies (Z) are dead processes whose parent has not destroyed them properly. They do not use any resources but can pollute output in <code>ps</code> and <code>top</code> and should be cleaned up.</p> <p><code>jobs</code> views and manages background jobs within the current shell session. <code>sleep</code> is an easy command to experiment with. After sleeping for a set amount of time, the process can be paused (SIGSTOP) with <code>ctrl + z</code> or immediately brought to the backround with <code>&amp;</code>.</p> <pre><code>~$ sleep 10\n^Z\n[1]+  Stopped                 sleep 10\n\n~$ sleep 20 &amp;\n[2] 3967600\n</code></pre> <p>View current jobs with <code>jobs</code> and bring one back to the foreground with <code>fg [id]</code>. By default <code>fg</code> will run the job marked with a <code>+</code>.</p> <pre><code>~$ jobs\n[1]-  Stopped                 sleep 10\n[2]+  Stopped                 sleep 20\n\n~$ fg 1\nsleep 10\n</code></pre> <p><code>ps</code> is a more general process monitoring tool for the whole system. Instead of viewing jobs only in the terminal session, you can detach from the terminal with the <code>x</code> flag and specify options/columns with <code>-o</code>. Here I only care about the PID and state.</p> <pre><code>~$ ps -x -o pid,state\n    PID S\n   1259 S\n   1260 S\n   1267 S\n</code></pre>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#nice-and-renice","title":"Nice and Renice","text":"<p>Nice is intended for batch or background jobs. Jobs are niced (given lower scheduling priority) so they don't use CPU when online users need it, and so foreground tasks have ample performance.</p> <p>Niceness value refers to CPU priority of userspace programs on a scale of -20 to 20, lower being higher priority. Use <code>nice</code> to launch a program with a specific niceness value, and <code>renice</code> to alter the priority of a currently running process. </p> <p>Here I launch my terminal with a relatively low priority of 10. Then I alter it's nice value to an even lower priority. </p> <pre><code>\u276f nice -n 10 gnome-terminal\n\u276f renice -n 15 9830\n</code></pre> <p>Keep in mind in order launch a program with high priority (below 0), you need root privilege. In the same manner, root privileged is also needed to increase a current running program's priority. </p>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#top","title":"top","text":"<p>Top is the default resource moniter preinstalled on nearly all Linux systems. You can think of it as a lightweight, terminal based Task Manager from Windows. Before jumping into it, you can list the running processes manually as well as find uptime and load average with simpler commands.</p> <p>The <code>ps</code> command by default will list out procceses ran in the terminal. The <code>x</code> flag lifts the restriction of the terminal and displays all ongoing processes in the system. The <code>u</code> flag adds a user column.</p> <pre><code>\u276f ps aux\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot           1  0.0  0.0 166648 12088 ?        Ss   07:23   0:01 /sbin/init splash\nroot           2  0.0  0.0      0     0 ?        S    07:23   0:00 [kthreadd]\nroot           3  0.0  0.0      0     0 ?        I&lt;   07:23   0:00 [rcu_gp]\n</code></pre> <p>We can also find uptime and load average with <code>uptime</code> or <code>cat /proc/loadavg</code>. These are explained below.</p> <pre><code>\u276f uptime\n 10:05:42 up  2:42,  1 user,  load average: 1.51, 1.13, 0.92\n\n~ \n\u276f cat /proc/loadavg \n1.46 1.12 0.92 1/1319 13289\n</code></pre> <p>For a more dynamic listing of this data, we can run <code>top</code>. You can think of this which as essentially running commands like <code>ps aux</code>, <code>uptime</code>, <code>free</code> for memory, and additional commands every three seconds, to actively moniter the system.</p> <p></p>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#statistics","title":"Statistics","text":"<p>Starting with the statistics at the top, starting from the left, you can see the up time, which is how long the system has been running. Moving right, the number of users is displayed, then we have load average, which is average CPU usage in the past 1 minute, 5 minutes, and 15 minutes. It is  based on your number of cores. For example if you have 4 cores, then a 4.0 would mean 100% CPU usage, in this screenshot, I have 2 cores so in the past 1 minute I had 63% usage of my 2 cores.</p>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#process-table","title":"Process Table","text":"<p>Now, let's go over each column in the process table.</p> <ul> <li>PID: Process ID.</li> <li>USER: The user running the process.</li> <li>PR: Priority of the task computed by the kernel on a scale of 20 to -20.</li> <li>NI: \"Niceness\" value, which involves the priority of user processes. 0 is the default and highest priority.</li> <li>VIRT: Virtual and physical memory, representing the total memory space allocated to a process, including RAM and swap. It's like a hypothetical maximum needed by the program. RES + Swap space = VIRT</li> <li>RES: Resident (Physical) memory used by the process. VIRT - Swap space = RES</li> <li>SHR: Shared memory.</li> <li>S: State of the process, where \"R\" means running, \"S\" means sleeping, and \"I\" is idle.</li> </ul> <p>To only display columns that matter to you, you can press <code>f</code> while in top for field management. In this case I only care about process state, cpu, memory, how long the process has been running, and the command name.</p> <p>Navigate with the arrows and select fields with the spacebar.  Now I have a much more minimal process table with only the fields I care about. </p>"},{"location":"1.%200%20System%20Management/1.4%20Processes%20and%20Services/#htop","title":"htop","text":"<p>The most popular alternative, <code>htop</code>, provides more customization, scrolling and mouse support, color, and an overall cleaner interface. Unlike <code>top</code>, it does not come preinstalled but is worth checking out with a quick download.</p> <pre><code>sudo apt install htop\n</code></pre> <p>I recommend to learn <code>top</code> first and then try out <code>htop</code>, comparing the two. </p> <p>Upon running <code>htop</code>, you will first realize the displayed columns discussed above are exactly the same, as well as the tasks, load average, and up time at the top right. The main difference is the colorful, more readable TUI (text user interface) that supports mouse events and scrolling. An example of this is the CPU column, colored blue after clicking, ordering the processes by CPU consumption. </p> <p>![[htop.jpg]]</p>"},{"location":"1.%200%20System%20Management/1.5%20Networking/","title":"1.5 Networking","text":"<p>Networking tools in Linux vary significantly across distributions.</p> <p>The newer iproute2 package includes <code>ss</code> and <code>ip</code> which largely replace commands such as <code>ifconfig</code>, <code>route</code>, and <code>netstat</code>.</p> <p>On the man page of the netstat command, you can read:</p> <pre><code>This program is mostly obsolete.  Replacement for netstat is ss. Replacement for  netstat -r  is ip route.  Replacement for netstat  i is ip -s link.  Replacement for netstat -g is ip maddr.\n</code></pre> <p>Like the man page says, the <code>ss</code> command has replaced the <code>netstat</code> command. Other deprecated programs in the net-tools package include:</p> <p><code>route</code> and <code>netstat -r</code> that have been replaced with <code>ip route</code></p> <p><code>arp</code> which have been replaced by <code>ip neigh</code> or neighbor.</p> <p>Some useful terminology to learn across these programs:</p> <p>inet - internet protocol family (IPv4) inet6 - the modern protocol of IP addresses represented in hexidecimal lo - virtual loop back device/interface for troubleshooting <code>ifconfig lo</code> wlo1 - wireless network interface (NIC) RX - Receive TX - Transmit</p>"},{"location":"1.%200%20System%20Management/1.5%20Networking/#network-monitoring","title":"Network Monitoring","text":""},{"location":"1.%200%20System%20Management/1.5%20Networking/#tshark","title":"Tshark","text":"<p><code>-i</code> for interface<code>-f</code> for capture filter</p> <pre><code>\u276f sudo tshark -i wlo1 -f \"src port 443\"\n</code></pre> <p>You can also read <code>-r</code> from and write <code>-w</code> to a file.</p>"},{"location":"1.%200%20System%20Management/1.5%20Networking/#tcpdump","title":"tcpdump","text":"<p>Similar to tshark, <code>tcpdump</code> can also analyze packets on a network.</p> <p>List interfaces with <code>-D</code>.</p> <pre><code>\u276f tcpdump -D\n1.wlo1 [Up, Running, Wireless, Associated]\n2.any (Pseudo-device that captures on all interfaces) [Up, Running]\n3.lo [Up, Running, Loopback]\n4.enp2s0 [Up, Disconnected]\n5.virbr0 [Up, Disconnected]\n6.docker0 [Up, Disconnected]\n</code></pre> <p>The syntax is very similar to tshark</p> <pre><code>\u276f sudo tcpdump -i wlo1 port 443 -w tcpdump.pcap\n</code></pre>"},{"location":"1.%200%20System%20Management/1.5%20Networking/#my-traceroute-mtr","title":"My Traceroute mtr","text":"<p>Article on <code>mtr</code> command. Combines <code>ping</code> and <code>traceroute</code> functionality.</p> <p><code>-r</code> report mode sends 10 packets in the background and write to sdout. Use <code>-c</code> to cycle the number of packets.</p> <pre><code>\u276f mtr -r -c 15 google.com &gt; report.txt\n</code></pre> <p>mtr uses ICMP echos by default, but you can use UDP with <code>-u</code> and TCP SYN packets with <code>-T</code>.</p>"},{"location":"1.%200%20System%20Management/1.5%20Networking/#checking-open-ports","title":"Checking Open Ports","text":"<p>You want set up a server for SSH, the first step is making sure port 22 is open. Here are the various ways to check.</p> <pre><code>ss -tuln | grep ':22'\ntelnet localhost 22\nsudo lsof -iTCP #look for a name with ssh\n</code></pre>"},{"location":"1.%200%20System%20Management/1.5%20Networking/#firewalls","title":"Firewalls","text":"<p>Let's say telnet refuses connection and the other commands do not find port 22, this could be UFW (Uncomplicated Firewall) is blocking it. UFW basically a wrapper over <code>iptables</code> for Ubuntu. <code>sudo ufw status</code> will let you know if it is enabled or not, and what ports it allows or blocks.</p> <p>UFW will either use <code>iptables</code> or <code>nftables</code> by default. Here we can see nf_tables is used, if it wasn't, it would say \"legacy\".</p> <pre><code>iptables -V\niptables v1.8.7 (nf_tables)\n</code></pre> <p>iptables is old and nftables if preferred.</p>"},{"location":"1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/","title":"1.6 Package Management and Sandboxed Apps","text":""},{"location":"1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#package-management","title":"Package Management","text":"<p>In the old days of Linux in the 90s, installing packages was only possible with tools like <code>dpkg</code> (Debian Package) and <code>rpm</code> (Red Hat Package Manager), which each install a single binary at a time. This meant you had to hunt down each remaining <code>.deb</code> or <code>.rpm</code> package dependency yourself. These would now be considered low-level package managers, since the creation of tools like <code>apt</code> (Advanced Package Manger) on Debian and Ubuntu, as well as <code>yum</code> and <code>dnf</code> on Red Hat. As high-level package managers, they will automatically resolve dependencies, often installing many packages at once for a single program. Additionally, these tools provide automatic updates and generally simplify the software installation process.</p> <p>These tools work by reading from a public database of packages called repositories. We can see the number of packages in the Apt repository with the word count program to count each line/package. As you can see, we have over 80 thousand packages to choose from.</p> <pre><code>\u276f apt list | wc -l\n81413\n</code></pre> <p>This package management system is not just for Linux Administration, but for software development across programming languages as well. Javascript has the Node Package Manager (NPM) which is a cornerstone for web development, while  Python has PIP (Package Installer for Python), which grabs packages from the Python Package Index (PyPi).</p> <p>Returning to Debian and Red Hat package managers, both <code>dpkg</code> and <code>rpm</code> have the same <code>-i</code> flag for installing.</p> <pre><code>\u276f dpkg -i package.deb\nOR\n\u276f rpm -i package.rpm\n</code></pre> <p><code>yum</code> is the older package manager for Red Hat, with <code>dnf</code> (Dandified Yum) being the modern tool which includes more features such as rollback and undo.</p> <p>Here is a great article about the history of package management.</p>"},{"location":"1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#sandboxed-apps","title":"Sandboxed Apps","text":""},{"location":"1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#appimages","title":"Appimages","text":"<p>Appimages are an entire app and all dependencies contained within a single file. This means there is no intermediary between the author and the user, such as required runtimes for Flatpacks and Snaps They are simple, distribution agnostic and do not require root. This is because appimages use FUSE (File System in Userspace), a system which allows non-root users to mount filesystems.</p> <p>To run an appimage, simply download an app from Appimagehub, give executable permissions with <code>chmod +x</code> and run.</p> <p>In this example, going to download Gun Mayhem 2.</p> <p> Once it's in my downloads folder, I'll give it executable permissions and play!</p> <pre><code>\u276f chmod +x Gun-Mayhem-2-x86-64.AppImage \n\u276f ./Gun-Mayhem-2-x86-64.AppImage \n</code></pre> <p> Now while the game is running, let's look at the temporary filesystem created with FUSE that the game is running on. We can see it is in a read-only (ro) mode for execution, ensuring isolation and security during its runtime. Once you close the AppImage application, this temporary filesystem will be unmounted automatically.</p> <pre><code>\u276f mount | grep Gun-Mayhem\n/home/promptier/Downloads/Gun-Mayhem-2-x86-64.AppImage on /tmp/.mount_Gun-MaPpsVle type fuse.Gun-Mayhem-2-x86-64.AppImage (ro,nosuid,nodev,relatime,user_id=1000,group_id=1000)\n</code></pre>"},{"location":"1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#flatpaks","title":"Flatpaks","text":"<p>Flatpaks isolate applications in a 'sandbox' by providing separate runtimes/platforms that are distribution agnostic. On my Linux Mint system, I run many applications in flatpaks for the sandboxed security and automatic updates they provide. List running flatpaks <code>ps</code>.</p> <pre><code>\u276f flatpak ps\nInstance   PID  Application           Runtime\n169358187  6282 com.brave.Browser     org.freedesktop.Platform\n2422727375 6242 com.brave.Browser     org.freedesktop.Platform\n2148086033 3309 md.obsidian.Obsidian  org.freedesktop.Platform\n3377083729 3268 md.obsidian.Obsidian  org.freedesktop.Platform\n739023738  2990 com.obsproject.Studio org.kde.Platform\n</code></pre> <p>To find more information about a particular application, use <code>info</code>.  ```  \u276f flatpak info com.brave.Browser </p> <p>Brave Browser - The web browser from Brave</p> <pre><code>      ID: com.brave.Browser\n     Ref: app/com.brave.Browser/x86_64/stable\n    Arch: x86_64\n  Branch: stable\n Version: 1.60.114\n License: MPL-2.0\n  Origin: flathub\n</code></pre> <p>Collection: org.flathub.Stable Installation: system    Installed: 368.6\u00a0MB      Runtime: org.freedesktop.Platform/x86_64/22.08          Sdk: org.freedesktop.Sdk/x86_64/22.08</p> <pre><code>\nWhether an app is currently running or not, I can list all installed flatpaks with `list`. \n</code></pre> <p>\u276f flatpak list Name                    Application ID                  Version           Branch      Installation Brave Browser           com.brave.Browser               1.60.114          stable      system Discord                 com.discordapp.Discord          0.0.35            stable      system OBS Studio              com.obsproject.Studio           30.0.0            stable      system Visual Studio Code      com.visualstudio.code           1.84.1-1699275408 stable      system Obsidian                md.obsidian.Obsidian            1.4.16            stable      system ```</p> <p>Flathub is the official distribution service for Flatpaks and provide easy installation of your favorite apps. In recent years, Flatpak has become a standard method of distributing software on Linux desktops</p> <p> Besides the Linux desktop, Flatpaks are are often used for distributing applications on servers as well.</p>"},{"location":"1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#snaps","title":"Snaps","text":"<p>Developed by Canonical as the standard software packaging and deployment method on Ubuntu, Snaps are comparable to Flatpaks as self contained, sandboxed apps. Like Flathub, the Snapstore is the official  distribution service for snap packages.   Though very popular now on Ubuntu desktop, Snap packages are primarily to distribute software across servers running Ubuntu. </p>"},{"location":"1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/","title":"1.7 Manage Software Configurations","text":""},{"location":"1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#configure-kernel-options","title":"Configure Kernel Options","text":""},{"location":"1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#kernel-modules-vs-drivers","title":"Kernel Modules vs Drivers","text":"<ul> <li>some consider a kernel module just a \"piece of software\" of any kind at the kernel level that provide additional/different functionality, while a driver is a KIND of module used for communicating with hardware </li> <li>drivers are more specific and fixed, they communicate directly to the hardware and often start at boot. Examples include sound drivers for audio devices, display drivers for graphics cards, and network drivers for network interfaces</li> <li>modules are loaded dynamically throughout runtime and provide. They are easy to install or remove without rebooting the system.  Sometimes for a PCI device, the driver and module are the same, such as on my ethernet interface</li> </ul> <pre><code>$ lspci -vs 00:1f.6\n00:1f.6 Ethernet controller: Intel Corporation Ethernet Connection (2) I219-LM (rev 31)\n    DeviceName:  Onboard LAN\n    Subsystem: Lenovo Ethernet Connection (2) I219-LM\n    Flags: bus master, fast devsel, latency 0, IRQ 122\n    Memory at df000000 (32-bit, non-prefetchable) [size=128K]\n    Capabilities: &lt;access denied&gt;\n    Kernel driver in use: e1000e\n    Kernel modules: e1000e\n</code></pre>"},{"location":"1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#ntp","title":"NTP","text":"<p>For system components to operate, such as Cronjobs and systemd timers to run at the proper time, there must be accurate timekeeping.  Computers can utilize the Network Time Protocol (NTP) to synchronize their time to an upstream time server or a pool of servers to maintain accurate time. It is a UDP service that runs on port 123.</p> <p>NTP is how millions of computer's clocks stay synchonized over a network. This involved many time servers utilizing algorithms to mitigate the effect of network latency. NTP can typically maintain time to within 10 milliseconds over the public internet.</p> <p>The stratum model is key in understanding how the precision of time degrades over a network. Stratum 0 are devices directly connected to the reference clock, such as a GPS antenna. Stratum 1 are time servers that distribute time to clients i.e Stratum 2 devices. The higher the Stratum number, the more the timing accuracy and stability degrades because the greater distance from the reference clock.</p> <p>Use the following command to check your system clock and the sync status. Though <code>status</code> is actually the default if you just run <code>timedatectl</code> by itself.</p> <pre><code>\u276f timedatectl status\n               Local time: Wed 2023-11-15 16:44:58 CST\n           Universal time: Wed 2023-11-15 22:44:58 UTC\n                 RTC time: Wed 2023-11-15 22:44:58\n                Time zone: America/Chicago (CST, -0600)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n</code></pre> <p>If NTP service is inactive, set it to true.</p> <pre><code>timedatectl set-ntp 1\n</code></pre> <p>Here we can look at the NTP server, what stratum level we are, delay, etc.</p> <pre><code>timedatectl timesync-status \n       Server: 2620:2d:4000:1::40 (ntp.ubuntu.com)\nPoll interval: 4min 16s (min: 32s; max 34min 8s)\n         Leap: normal\n      Version: 4\n      Stratum: 2\n    Reference: C944586A\n    Precision: 1us (-25)\nRoot distance: 1.715ms (max: 5s)\n       Offset: +7.926ms\n        Delay: 139.890ms\n       Jitter: 2.988ms\n Packet count: 4\n</code></pre>"},{"location":"1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#chrony","title":"Chrony","text":"<p>Chrony is a flexible NTP implementation to sync your clock across various NTP servers. It's deamon, <code>chronyd</code> continuously adjusts the system clock to calculate offset and drift.</p> <p>Here is a guide from Ubuntu on installing and configuring Chrony. </p>"},{"location":"1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#localectl","title":"localectl","text":"<p>This command is for managing your keyboard configuration and language settings.</p> <pre><code>\u276f localectl\n   System Locale: LANG=en_US.UTF-8\n                  LANGUAGE=en_US:en\n       VC Keymap: n/a\n      X11 Layout: us\n       X11 Model: pc105\n</code></pre> <p>As an example, you could set the keymapping to a US layout with <code>localectl set-keymap us</code>.</p> <p>Here you can display locale names available on your system, which determines the language, character encoding, and cultural conventions used for displaying and formatting text.</p> <pre><code>\u276f localectl list-locales \nC.UTF-8\nen_AG.UTF-8\nen_AU.UTF-8\nen_BW.UTF-8\nen_CA.UTF-8\nen_DK.UTF-8\nen_GB.UTF-8\n</code></pre>"},{"location":"2.0%20Security/2.1%20PKI-Certs-Auth-Linux%20Hardening/","title":"2.1 PKI Certs Auth Linux Hardening","text":"<p>These are the exam objectives planned.</p> <p>Managing public key infrastructure (PKI) certificates - Public key - Private key - Self-signed certificate - Digital signature - Wildcard certificate - Hashing - Certificate authorities \u2022 Certificate use cases - Secure Sockets Layer (SSL)/ Transport Layer Security (TLS) - Certificate authentication - Encryption \u2022 Authentication - Tokens - Multifactor authentication (MFA) - Pluggable authentication modules (PAM) - System Security Services Daemon (SSSD) - Lightweight Directory Access Protocol (LDAP) - Single sign-on (SSO) \u2022 Linux hardening - Security scanning - Secure boot \u2022 UEFI - System logging configurations - Setting default umask - Disabling/removing insecure services - Enforcing password strength - Removing unused packages - Tuning kernel parameters - Securing service accounts - Configuring the host firewall</p> <p>Have notes of any of the following? Feel free to contribute!</p>"},{"location":"2.0%20Security/2.2%20Identity%20Management/","title":"2.2 Identity Management","text":"<p>bashrc - Hidden file within each users home directory executed for non-login shells. Often stores aliases and other customization. Here is an example of mine.</p> <pre><code># Useful aliases\nalias home='cd ~'\nalias cd..='cd ..'\nalias ..='cd ..'\nalias ...='cd ../..'\n\n# Set vi mode instead of default emacs based bash commands\nset -o vi\n\n# Get weather from the terminal\nalias wtr=\"curl wttr.in\"\n\n# Starship faunt\neval \"$(starship init bash)\"\n</code></pre> <p>bash_profile or profile - Another hidden file in a users home directory executed for login shells. Instead of the bashrc which will run every time you open a terminal, this will only run once when you login. </p> <p>Keep in mind there is also system wide profile and bashrc files located within <code>/etc</code> for all users.</p> <p>/etc/login.defs - where commands like useradd, usermod, groupadd and others take values, such as min and max value for user or group id. Other functions like password aging controls, login retry attempts, and permissions/mode of new home directories with umask. Some functions like login retry attempts can be overridden by PAM and others obsoleted. </p> <p>pam_tally2 - <code>pam_tally2</code> has been deprecated in PAM 1.5.0 (2020) in favor of <code>pam_faillock</code></p> <p>faillock is a module for locking out users after failed attempts. It is configured in <code>/etc/security/faillock.conf</code>. Here you can adjust number of attempts and length of lockout time. </p> <p>In this setup, 3 failed login attempts within 15 minutes of each other will lock that user out for 10 minutes.</p> <pre><code>deny = 3\n\n# The default is 900 (15 minutes).\nfail_interval = 900\n\n# The default is 600 (10 minutes).\nunlock_time = 600\n</code></pre>"},{"location":"2.0%20Security/2.3%20Firewalls/","title":"2.3 Firewalls","text":"<p>Here are the exam objectives planned.</p> <p>Firewall use cases - Open and close ports - Check current configuration - Enable/disable Internet protocol (IP) forwarding \u2022 Common firewall technologies - firewalld - iptables - nftables - Uncomplicated firewall (UFW) Key firewall features - Zones - Services - Stateful - Stateless</p> <p>Have notes of any of the following? Feel free to contribute!</p>"},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/","title":"2.4 SSH and Excecuting Commands as Another User","text":""},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#ssh","title":"SSH","text":""},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#history-and-overview","title":"History and Overview","text":"<p>SSH is a protocol developed by a Finnish researcher in 1995 after his university was attacked by a password sniffer. At the time insecure protocols like telnet were used. SSH encrypts all data over a network, so hackers can't do any sniffing.</p> <p>Two things are needed to connect, username for which user you want to login as, and the server host, which can either be an IP address or domain name. </p> <pre><code>ssh username@serverhost\n</code></pre>"},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#key-based-authentication","title":"Key Based Authentication","text":"<p>By default, a server will have password-based authentication. Key-based authentication is recommended with a public and private key.</p> <p><code>ssh-keygen</code> is primarily used on the client to generate key pairs for authentication when connecting to servers.</p> <p>ed25519 is the recommended algorithm for SSH key generation. As of OpenSSH 9.5 ed25519 is used by default for <code>ssh-keygen</code></p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/[filename] C \"[Comment]\"\n</code></pre> <p>An optional passphrase will be presented.</p> <p>This will create both <code>filename.pub</code> with the public key and comment, as well as <code>filename</code> which will contain the private key.</p> <p>Now the public key needs to be sent to the server with <code>ssh-copyid</code>. <code>-i</code> is for identify file.</p> <pre><code>ssh-copyid -i ~/.ssh/filename.pub joseph@x.x.x.x\n</code></pre> <p>This will ask for the password, then add the client's public key to the servers <code>~/.ssh/authorized_keys</code> for easy login thereafter.</p> <p>The client's <code>~/.ssh/known_hosts</code> file contains fingerprints of servers public keys so that it can recognize them as trusted for future logins.</p> <p>If you have your keys generated and able to login with the machines you want, you should then disable password-based SSH authentication on the server. </p> <p>This is the server's system-wide sshd config file </p> <pre><code>sudo vi /etc/ssh/sshd_config\n</code></pre> <p>Change or uncomment these</p> <pre><code>PasswordAuthentication no\nPubkeyAuthentication yes\nPermitRootLogin no\n</code></pre> <p>now reload the ssh deamon</p> <pre><code>sudo systemctl reload sshd\n</code></pre> <p>Now if you try to SSH into the server on any machine, it will be denied unless keys are generated and copied over like we did before.</p> <p>Other security measures could be changing the SSH port away from the default 22.</p> <p><code>/etc/ssh/sshd_config</code> is the server config, and <code>/etc/ssh/ssh_config</code> is the client config</p> <p><code>ssh_config</code> often comes by default so any computer can act as a client easily. HOWEVER, most systems will not have an <code>sshd_config</code> by default. For a system to act as a server, it needs this file.</p>"},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#ssh-agent-vs-systemd-sshd-service","title":"ssh-agent vs systemd sshd service","text":"<p><code>ssh-agent</code> is a program for clients that stores private keys in memory and is used for public-key authentication. Keys are added using <code>ssh-add</code> or when it is automatically configured in <code>ssh-config</code>. SSH looks at the environment variables below and uses it to establish a connection.</p> <p>Here is a look at one of my laptops which act as clients to connect ot my Ubuntu webserver.</p> <pre><code>\u276f ssh-agent\nSSH_AUTH_SOCK=/tmp/ssh-XXXXXXDHLZVi/agent.5315; export SSH_AUTH_SOCK;\nSSH_AGENT_PID=5316; export SSH_AGENT_PID;\necho Agent pid 5316;\n\n\u276f ps aux | grep ssh\njoe         4863  0.0  0.0   7972  1080 ?        Ss   11:08   0:00 ssh-agent\n</code></pre> <p>Now on the server side, here is my Ubuntu machine I use to run my Apache webserver. You can see the various Systemd services running to authenticate clients.</p> <pre><code>\u276f systemctl status ssh\nsshd-keygen.service  ssh.service          \nsshd.service         ssh.socket      \n</code></pre> <p>So to summarize, <code>ssh-agent</code> is typically used on the client side to manage SSH keys, while SSH Systemd services like <code>sshd</code> are usually found on the server side to accept incoming SSH connections.</p>"},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#executing-commands-as-another-user","title":"Executing commands as another user","text":""},{"location":"2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#policy-kit-and-pkexec","title":"Policy Kit and pkexec","text":"<p>Polkit is the \"sudo of systemd\". It defines the policy that allows unprivileged processes to speak to privileged processes, such as granting a user the right to perform some task in a certain context. Instead of granting root permission of the entire process, like <code>sudo</code>, polkit instead allows more fine grain control, such as privileges for certain situations or duration of time. </p> <p>As an alternative to running a command as root or superuser privilege, you can use <code>pkexec</code>. This is common for when you screw up your <code>/etc/sudoers</code> file.</p> <p>It will prompt you for your password if on a text-based environment and window if in a graphical session.</p> <pre><code>\u276f pkexec ls\n==== AUTHENTICATING FOR org.freedesktop.policykit.exec ===\nAuthentication is needed to run `/usr/bin/ls' as the super user\nAuthenticating as: Jobo Baggins (joe)\nPassword: \n</code></pre> <p> If failed, the incident will be reported and you can see the logs in <code>/var/log/auth.log</code></p> <pre><code>Dec 13 18:32:47 statsfordevs pkexec[853597]: joe: Error executing command as another user: Not authorized [USER=root] [TTY=/dev/pts/0] [CWD=/home/joe] [COMMAND=/usr/bin/ls]\nDec 13 18:32:47 statsfordevs polkitd(authority=local): Unregistered Authentication Agent for unix-process:846897:34748679 (system bus name :1.156, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_US.UTF-8)\n\n</code></pre>"},{"location":"2.0%20Security/2.5%20Access%20Controls/","title":"2.5 Access Controls","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#acl","title":"ACL","text":"<p>By default, Linux permissions only allow a single user and single group of a given directory/file. We can see this text file is owned by Joe and belongs to the \"study\" group.</p> <pre><code>ls -l test.txt \n-rw-r----- 1 joe study 8 Nov 30 19:33 test.txt\n</code></pre> <p>If we wanted more flexible permissions, such as a user that is not Joe nor part of the study group to be able to read and write to the file, we can use Access Control Lists (ACL). These are great for giving multiple groups access to a file or directory, or giving a specific user access even though they are not part of the file's group.</p> <p><code>getfacl</code> and <code>setfacl</code> are our two go-to commands (the f in the commands are\"file\"). Use the former to view the current permissions of a file. Here we have the default permission setup of a single user and group.</p> <pre><code>getfacl test.txt \n# file: test.txt\n# owner: joe\n# group: study\nuser::rw-\ngroup::r--\nother::---\n</code></pre> <p>Now lets add the work group by modifying <code>-m</code> the file and check it afterwords.</p> <pre><code>setfacl -m g:work:rw test.txt \ngetfacl test.txt \n# file: test.txt\n# owner: joe\n# group: study\nuser::rw-\ngroup::r--\ngroup:work:rw-\nmask::rw-\nother::---\n</code></pre> <p>Now not only is the work group added for read and write permissions, but this created a mask. This is the maximum effective permissions for any entry in the ACL.</p>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#attributes","title":"Attributes","text":"<p>Common permission attributes include append-only and immutability. Append-only prevents modifying existing content in a file, but only appending. Immutable files/directories cannot be written, appended, or deleted, even by root. </p> <pre><code># change attribute to immutable\nchattr -i [file]\n# change attribute to append-only\nchattr -a [file]\n</code></pre> <p>To list attributes of files in current directory, or point to a file/directory, use <code>lsattr</code>.</p> <p>To check whether a file has either ACL permission set, attributes assigned, or both, look for the plus at the end of the permissions with <code>ls -l</code>. </p> <pre><code>#         v\n-rw-rw----+  1 joe study     8 Nov 30 19:33 test.txt\n</code></pre>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#special-permissions","title":"Special Permissions","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#set-user-id-suid","title":"Set user ID (SUID)","text":"<p>When a program is ran with SUID, it executes with the permissions of whoever owns the file. In the case of <code>umount</code>, if a normal user were to execute this command, they would in fact run it as if they were root. This is generally for programs that require system access that you still want users to run.</p> <pre><code>#SUID bit set\n   v\n-rwsr-xr-x 1 root root       35192 Feb 20  2022 umount\n</code></pre> <p>Note if the file owner does not have execute permissions, there would be an uppercase S .</p> <p>Another common example of SUID is the <code>passwd</code> command. Any user should be able to change their password and this is possible by executing the file as if they were root.</p> <pre><code>ls -l /bin/passwd\n-rwsr-xr-x 1 root root 59976 Nov 24  2022 /bin/passwd\n</code></pre> <p>Adding this permission to a file is as follows.</p> <pre><code>chmod u+s &lt;filename&gt;\n</code></pre>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#set-group-id-sgid","title":"Set group ID (SGID)","text":"<p>SGID does the same thing but for running a program with permissions of the group. Here we can see the group of executable file <code>ssh-agent</code> is <code>_ssg</code>. When a user is running this program, they will act as a member of the file's group.</p> <pre><code>#SGID bit set\n      v\n-rwxr-sr-x 1 root _ssh      293304 Aug 24 08:40 ssh-agent\n ```\n SGID has a different function on directories. If set, any file created within that directory will inherit the group ownership of the parent directory group.\n\nFor example, this directory has an owner of bill and group of study. I can add SGID with the following. Note the before and after in the executable portion of the group permissions.\n</code></pre>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#before","title":"Before","text":"<pre><code>  v\n</code></pre> <p>drwxrwxr-x 2 bill study 4096 Dec  3 12:24 dir</p> <p>chmod g+s sgid</p>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#after","title":"After","text":"<pre><code>  v\n</code></pre> <p>drwxrwsr-x 2 bill study 4096 Dec  3 12:25 dir</p> <pre><code>\n`file1` was created by joe before adding the SGID. After it was set `file2` was created inheriting the study group from it's parent directory. \n</code></pre> <p>ls -l dir total 0 -rw-rw-r-- 1 joe joe   0 Dec  3 12:25 file1 -rw-rw-r-- 1 joe study 0 Dec  3 12:26 file2</p> <pre><code>##### Sticky bit\nFolders with this activated cannot have files within them deleted or renamed by other users. This is commonly used on directories like `/tmp` to prevent users from deleting each other's files. \n</code></pre>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#tmp-folder-has-sticky-bit-set-on-by-default","title":"tmp folder has sticky bit set on by default","text":"<pre><code>     v\n</code></pre> <p>drwxrwxrwt  17 root root       4096 Dec  3 11:44 tmp</p> <pre><code>Use the following either in symbolic or numeric mode to add this restriction. \n</code></pre> <p>chmod +t dir</p>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#or","title":"OR","text":"<p>chmod 1777 dir</p> <pre><code>Though it is possible to add sticky bit to a file, it currently has no function in modern Linux systems.\n\nSo what to remember about each 3 of these permissions is that SUID is for executable files only, SGID works on both files and directories, and sticky bit only works on directories.\n\n#### Apparmor\nApparmor is a Linux kernel security module found in Debian based and Ubuntu systems. It provides Mandatory Access Control (MAC) to restrict programs, applications, and scripts capabilities with per-program profiles. It is considerably easier than SELinux setup and maintain. Unlike SELinux, which is based on applying labels to files based on their inodes, AppArmor works with file paths.\n\n`aa-status` will display profiles of programs either in enforce mode, which will log and block access, and complain mode, which will only log. You can view these logs in `/var/log/syslog`.\n\nInstalling `apparmor-utils` includes essential commands such as those needed to move profiles to enforce or complain mode, and to generate profiles.\n\nIn the `/etc/apparmor.d` directory, you will find the profiles on your system as seen with `aa-status`. To create a profile with an easy template, use `aa-easyprof`, or `aa-genprof` to create your own.\n\nTo restrict the scope of access of the `ss` program, I can create a profile and place it in the apparmor directory as follows. Next I will enforce it.\n</code></pre> <p>\u276f sudo aa-easyprof /usr/bin/ss &gt; usr.bin.ss \u276f sudo mv usr.bin.ss /etc/apparmor.d/ \u276f sudo aa-enforce /etc/apparmor.d/usr.bin.ss </p> <pre><code>At first, the program will essentially be disabled until you explicitly define it's permissions. \n\nWhen I try to run `ss`, it is denied permission. These logs can then be found in `var/log/syslog`. \n</code></pre> <p>\u276f ss Cannot open netlink socket: Permission denied Cannot open netlink socket: Permission denied Cannot open netlink socket: Permission denied \u276f tail -1 /var/log/syslog Dec  9 15:42:56 x230-2 kernel: [15915.826222] audit: type=1400 audit(1702158176.819:103): apparmor=\"DENIED\" operation=\"create\" profile=\"/usr/bin/ss\" pid=16843 comm=\"ss\" family=\"netlink\" sock_type=\"raw\" protocol=4 requested_mask=\"create\" denied_mask=\"create\"</p> <pre><code>`aa-logprof` is an interactive tools that reads `var/log/syslog` for Apparmor events and will generate a list of suggested profile changes that the user can choose from to update security profiles. For example, if I recently ran `ss` when after I restricted it, it would prompt me to adjust permissions.\n</code></pre> <p>\u276f aa-logprof  Updating AppArmor profiles in /etc/apparmor.d. Reading log entries from /var/log/syslog. Complain-mode changes:</p> <p>Profile:        /usr/bin/ss Network Family: netlink Socket Type:    raw</p> <p>[1 - include ]   2 - network netlink raw,  (A)llow / [(D)eny] / (I)gnore / Audi(t) / Abo(r)t / (F)inish <p>...</p> <p>(S)ave Changes / Save Selec(t)ed Profile / [(V)iew Changes] / View Changes b/w (C)lean profiles / Abo(r)t Writing updated profile for /usr/bin/ss.</p> <pre><code>\nAfter I have allowed access to multiple processes and saved changes to the profile. I can see the change in `usr.bin.ss`. The default `aa-easyprof` template it started with basically allows nothing, then afterwords we can see the what we allowed from `aa-logprof`.\n\nBefore `aa-logprof` (default `aa-easyprof`, nothing)\n</code></pre> <p>\u276f cat /etc/apparmor.d/usr.bin.ss </p>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#vimsyntaxapparmor","title":"vim:syntax=apparmor","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#apparmor-policy-for-ss","title":"AppArmor policy for ss","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#author","title":"###AUTHOR","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#copyright","title":"###COPYRIGHT","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#comment","title":"###COMMENT","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#include","title":"include","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#no-template-variables-specified","title":"No template variables specified <p>/usr/bin/ss {   #include  <p># No abstractions specified   # No policy groups specified   # No read paths specified   # No write paths specified }</p> <pre><code>After allowing access to multiple processes using `aa-logprof`\n</code></pre> <p>\u276f cat /etc/apparmor.d/usr.bin.ss </p>","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#last-modified-sat-dec-9-172136-2023","title":"Last Modified: Sat Dec  9 17:21:36 2023 <p>include","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#vimsyntaxapparmor_1","title":"vim:syntax=apparmor","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#apparmor-policy-for-ss_1","title":"AppArmor policy for ss","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#author_1","title":"###AUTHOR","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#copyright_1","title":"###COPYRIGHT","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#comment_1","title":"###COMMENT","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#no-template-variables-specified_1","title":"No template variables specified <p>/usr/bin/ss {   include    include  <p>/proc//net/raw r,   /proc//net/udp r,   /proc/*/net/unix r, }</p> <pre><code>#### SELinux\nSecurity Enhanced Linux is a Mandatory Access Control (MAC) system built into the upstream Linux kernel itself. It was originally developed by the NSA and Red Hat as a series of security patches using Linux Security Modules (LSM).  \n\nTwo most important concepts in SELinux are Labeling and Type Enforcement.\n\nFiles, directories, ports, etc. are all labelled with a SELinux context. Files and directories' labels are stored as extended attributes on the filesystem. Processes and port's labels are managed by the kernel.\n\nLabels are in this format: \n- user:role:**type**:level(optional)\n\nUse the `-Z` flag to create files and directories with SELinux security context, or `ls` to print security context.\n</code></pre> <p>cp -Z mkdir -Z ls -Z</p> <pre><code>Use `chcon` and `restorecon` to change context of a file. Note changes with chcon do not survive a filesystem relabel, often triggered with `/.autorelabel` at boot time.\n\nFile's contexts are set when they are created based on their parent directories context (with some exceptions). File context and label's refer to the same thing.\n\nLet's say some file is not working and you suspect it is the context after inspecting it with `ls -Z`. If you have a known good file you can reference it's context and point it at the file to copy it's context.\n</code></pre> <p>chcon --reference known-good.txt broken.txt  ```</p> <p>You can also restore a file or directory to the default context using <code>restorecon</code>. </p> <p>To change the default setting, i.e persistent changes use <code>semanage fcontext</code>. Then run resorecon to apply the changes or run an autorelabel. </p>","text":""},{"location":"2.0%20Security/2.5%20Access%20Controls/#dac-mac-discretionary-vs-mandatory-access-control","title":"DAC &amp; MAC - Discretionary vs Mandatory Access Control","text":"<p>Traditional Unix/Linux systems were discretionary. These have simple owner, group and other permissions. A user has the ability (discretion) to change permissions on their own files and <code>chmod +rwx</code> his own home directory and nothing will stop them, and then nothing will stop other users or processes from accessing that user's directory. On top of this Discretionary Access Control gives the root user omnipotent powers.</p> <p>On Mandatory Access Control systems, there is a policy which is administratively set and fixed. Even if you change the DAC settings on your home directory, if there is a policy in place which prevents another user or process from accessing it, you're generally safe.</p> <p>These policies can be very fine grained. Policies can be set  to determine access between:  - Users - Files - Directories - Memory - Sockets - tcp/udp ports - etc.</p>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#selinux-and-apparmor-comparison","title":"SELinux and Apparmor Comparison","text":"<ul> <li>Selinux operates at the system level, while Apparmor works at the applications level.</li> <li>Apparmor identifies filesystem objects by filepath as opposed to Selinux which identifies them by inode.</li> <li>Selinux is used by RHEL(Red Hat), SLES(SUSE), and Android, while Apparmor is used by Debian and Ubuntu.</li> </ul> <p>Application Level Apparmor - Deny Everything Selinux - Allow Everything System Level Apparmor - Allow Everything Selinux - Deny Everything</p>"},{"location":"2.0%20Security/2.5%20Access%20Controls/#comparison-of-selinux-and-apparmor","title":"Comparison of SELinux and AppArmor","text":"Feature SELinux AppArmor Original Sponsor DoD/NSA (USA) DoD/DARPA (USA) Approach Deny everything system level Deny everything application level Integration in Linux LSM Module + Userland LSM Module + Userland Performance Impact &lt;5% 0-2% Restriction MAC, RBAC, MLS, Network Labeling MAC, RBAC, (MLS) Confinement Indirect, via \"Labels\" Direct, Path based Profiles/Policies \"Program\" needs to be compiled Simple text files, Unix style Auditing / Human SELinux policies are hard to read and to audit AppArmor profiles are easy to audit Availability in Distros CentOS, RHEL, SUSE Linux Enterprise Ubuntu, openSUSE, SUSE Linux Enterprise Primary Adoption US/Canada Worldwide Notes SUSE Linux Enterprise - Support for software stack, no policy included in SLES. Preconfigured part of SLE Micro 5. Support for software stack and AppArmor profiles. Will be used for CCC for SLE 15. Courtesy SUSECON #### Additional Learning SUSE Conference Overview of SELinux and AppArmor Presentation Red Hat Summit Security-Enhanced Linux for mere mortals Presentation Very in depth interview with Selinux/NSA Engineer Interview Red Hat Selinux Documentation"},{"location":"2.0%20Security/Permissions-Octal%20Values/","title":"Permissions Octal Values","text":"<ul> <li>r (read): 4</li> <li>w (write): 2</li> <li>x (execute): 1 </li> </ul> <p><code>chmod 764 file</code> would mean rwx for owner, rw for group, and r for others.</p> <p>umask is the default set of permissions that subtracts from 666 for files, and 777 for directories</p> <pre><code>umask\n0002\n\ntouch file\n\nstat -c \"%a\" file\n664\n</code></pre> <p><code>chmod</code> can use octal mode/numeric (like above) or symbolic.</p> <p>Here we change the file permissions for owner and group to rwx</p> <pre><code>chmod og+rwx file\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/","title":"3.1 Bash scripting","text":""},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#brace-expansion","title":"Brace Expansion","text":"<pre><code>\u276f echo {1..5}\n1 2 3 4 5\n\u276f echo {0..20..2}\n0 2 4 6 8 10 12 14 16 18 20\n\u276f echo {a..f}\na b c d e f\n\n\u276f touch file{1..4}.txt\n\u276f ls\nfile1.txt  file2.txt  file3.txt  file4.txt \n\n\u276f cp -p file.config{,.bak}\n\u276f ls\nfile.config  file.config.bak\n\n\u276f echo {contents,paper,bilbiography}.md\ncontents.md  paper.md  bilbiography.md\n\n\u276f wget https://www.some-url.com/picture{1..8}.jpg\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#command-substitution","title":"Command Substitution","text":"<p>Capture the output of a command, and store it in a variable.</p> <pre><code>var=$(command)\n</code></pre> <p>or</p> <pre><code>var=`command`\n</code></pre> <pre><code>my_date=`date +%m-%d-%Y`\n#OR\nmy_date=$(date +%m-%d-%Y)\necho \"You accessed this date on $my_date\"\n</code></pre> <p>Output</p> <pre><code>You accessed this date on 09-13-2023\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#globbing","title":"Globbing","text":"<p>Globbing in Bash refers to how the shell interprets special characters such as <code>*</code>, and <code>?</code> which are commonly used to match filenames. Bash itself cannot recognize regular expressions, instead we use tools like <code>sed</code> and <code>awk</code>. It is important to distinguish globbing and regex as they can be easily confused.</p> <p>The question mark matches any single character while the asterisks matches zero or more characters.</p> <pre><code>\u276f ls file?.txt\nfile1.txt  file2.txt  file3.txt file4.txt file5.txt\n\n\u276f ls file[1-3].txt\nfile1.txt  file2.txt  file3.txt\n\n\u276f ls *.txt\ndep.txt  file1.txt  file2.txt  file3.txt  file4.txt file5.txt   marks.txt\n\n\u276f ls file?+(.png|.txt)\nfile1.png  file1.txt  file2.png  file2.txt  file3.png  file3.txt  file4.png  file5.png\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#here-documents","title":"Here Documents","text":"<p>Send multiple lines of text to a command or shell script.</p> <pre><code>command &lt;&lt; [marker]\ninput\n[marker]\n</code></pre> <pre><code>\u276f sort &lt;&lt; END\n\u2219 3\n\u2219 2\n\u2219 4\n\u2219 1\n\u2219 END\n1\n2\n3\n4\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#here-string","title":"Here String","text":"<p>Send one line of text to a command or shell script.</p> <pre><code>\u276f wc -c &lt;&lt;&lt; \"String with many characters\"\n28\n\n\u276f foo=\"bar\"\n\u276f sed 's/a/A/' &lt;&lt;&lt; \"$foo\"\n bAr\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#shell-control-and-redirection","title":"Shell control and redirection","text":"<p><code>&gt;&amp;</code>, <code>&amp;&gt;</code>, <code>&gt;&gt;&amp;</code> and <code>&amp;&gt;&gt;</code> : (read above also) Redirect both standard error and standard output, replacing or appending, respectively.</p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#additional","title":"Additional","text":"<p><code>source</code> reads and executes the contents of a file as sets of commands in the current shell.</p> <p>Here is a file called commands.txt</p> <pre><code>echo \"Your current directory is `pwd`\"\necho \"The date is `date +%m-%d`\"\n</code></pre> <pre><code>\u276f source commands.txt\nYour current directory is /home/promptier/Desktop/bash\nThe date is 11-20\n</code></pre> <p>Note <code>source script</code> is equivalent to <code>. script</code>, not to be confused with <code>./script</code>, which runs the script as an executable file, launching a new shell to run it.</p> <p>The <code>type</code> command is useful to learn more about a command. If it is a shell built in, it will not have a man page, instead, read about it using <code>man bash</code>.</p> <pre><code>\u276f man source\nNo manual entry for source\n\n\u276f type source\nsource is a shell builtin\n</code></pre> <p>The <code>$PATH</code> variable contains a list of directories the system checks before running a command. Instead of running <code>/usr/bin/python3</code>, we can just run <code>python3</code> because <code>/usr/bin</code> is located in the path.</p> <p>If you install a program that is not located in the <code>$PATH</code> variable, you can add it with either of the following two ways: </p> <pre><code>export PATH=/the/file/path:$PATH $\n#adds to the beginnning and will be checked first\n#OR\nexport PATH=$PATH:/the/file/path\n#adds to the end and will be checked last\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#script-utilities","title":"Script Utilities","text":"<p>By default, <code>tee</code> will overwrite files, use <code>-a</code> to append: </p> <pre><code>command | tee -a file\n</code></pre> <p>Tee can also be useful when writing to a protected file.</p> <pre><code>echo \"newline\" | sudo tee -a secret.txt\n</code></pre> <p>It is called \"tee\" as it resembles the letter \"T\" as well as the T-splitter in plumming since it takes from STDIN and \"splits\" or writes to both STDOUT and files. </p> <p><code>egrep</code>, <code>fgrep</code> and <code>rgrep</code> are the  same  as  <code>grep -E</code>, <code>grep -F</code>, and <code>grep -r</code>.</p> <p><code>grep -F</code> is a fixed string, meaning you want the string to be passed verbatim, and not interpreted as special regex. Such as if the search includes a dot <code>user.txt</code> that you don't want to be interpreted as a regex wildcard.</p> <p><code>grep -E</code> is extended grep, which can be used for fancy expressions, like <code>()</code> for groups and <code>|</code> for OR. Here we search for any line that starts with \"fork\" or \"group\".</p> <pre><code>\u276f grep -E '^no(fork|group)' /etc/group\nnogroup:x:65534:\n</code></pre> <p>If you used regular grep without the -E, you'd have to escape out the special characters or else they'd be searched literally.</p> <pre><code>grep '^no\\(fork\\|group\\)' /etc/group\n</code></pre> <p>As another useful example, we search for PCI devices starting with either \"ethernet\" or \"network\" with an insensitive search.</p> <pre><code>\u276f lspci | egrep -i 'ethernet|network'\n00:19.0 Ethernet controller: Intel Corporation 82579LM Gigabit Network Connection (Lewisville) (rev 04)\n03:00.0 Network controller: Intel Corporation Centrino Advanced-N 6205 [Taylor Peak] (rev 34)\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/","title":"3.5 Cloud & Orchestration Concepts","text":""},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/#kubernetes-vs-docker-compose","title":"Kubernetes vs Docker Compose","text":"<p>Docker compose is mainly for managing multi container applications on a single-host deployment, and lack the scalability features of kubernetes</p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/#cloud-init","title":"Cloud Init","text":"<p>Set of scripts and utilities used for configuring and customizing virtual machines when they first boot in a cloud environment, such as AWS and Azure. When the VMs boot, you can automatically configure the host name, install packages, add users and groups, configure SSH keys, really anything you want. This makes it easy to spin up VMs that are already configure how you want them. Cloud-init usually only runs on the first boot of a VM unless manually configured otherwise.</p> <p>Everything can be found in <code>/etc/cloud</code>. </p> <pre><code>\u276f ls /etc/cloud\nclean.d  cloud.cfg  cloud.cfg.d  cloud-init.disabled  ds-identify.cfg  templates\n</code></pre> <p>Five stages of Cloud-Init 1. Generator (determines if ) 2. Local 3. Network 4. Config 5. Final</p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Docker/","title":"Docker","text":"<p><code>docker run</code> = <code>docker create</code> + <code>docker start</code></p> <p>A stopped container can be restarted with <code>start</code>, unlike an exited container. <code>ps</code> will show only running containers, add the <code>-a</code> flag to include stopped ones. <code>docker ps -l</code> will who only the most recently created container, though in recent versions this may be <code>-n 1</code>, where you can manipulate the ouput, such as listing the last 5 created containers with <code>docker ps -n 5</code>. Use <code>-q</code> for \"quiet\" mode that will only list ids.</p> <pre><code>\u276f docker ps -n 3 -q\nf77c2b719993\n5b0b857c73eb\n807f5b547c1a\n</code></pre> <p>The main difference between the paused and stopped (exited) states is that the memory portion of the state is cleared when a container is stopped, whereas, in the paused state, its memory portion stays intact.</p> <p></p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Git/","title":"Git","text":"<p>Recently, Github has changed the primary branch name from the traditional \"master\" to \"main\". This is becoming the new standard but they still refer to the same thing. </p> <p><code>git branch [name]</code> - create new branch  <code>git checkout [name]</code> - move to branch Pull Request - Proposal to merge a set of changes from one branch to another, must be reviewed then approved. Often provided by hosting services like Github or Gitlab rather than a feature of Git itself. (Gitlab refers to them as Merge Requests) fireship HEAD - most recent commit</p> <p>Say you've  been working on a new feature in a branch called <code>feaure</code>, once completed, you can checkout to the main branch and merge the changes.</p> <pre><code>git checkout main\ngit merge feature\n</code></pre> <p></p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#git-tags","title":"Git Tags","text":"<p>By default, a tag will be lightweight, which includes no metadata. Make it annotated with the <code>-a</code> flag to include who tagged it and when. These annotated tags are considered best practice as it is nice to know when a version was released and who released it.</p> <p>Include a message with <code>-m</code> or it will launch an editor for you to do it anyways.</p> <pre><code>git tag -a v1.0 -m \"first release\"\n</code></pre> <p>Then you can see the release info with <code>git show v1.0</code></p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#rebase","title":"Rebase","text":"<p>Git rebase is an alternative to merge as a tool for integrating changes from one branch to another. Unlike merge, rebase has powerful history rewriting features. This is to maintain a linear and project history or in other words, a clean commit history.</p> <p>Note how the changes of the feature branch were moved on to the top of main. This flattens the history, removing unwanted entries.</p> <p></p> <p>The golden rule of git rebase is to never use it on public branches.</p> <p>To delete a specific commit, rebase in interactive mode just before the commit you want to delete, then delete the line containing that commit.</p> <pre><code>git rebase -i &lt;commit&gt;~1\n#or possibly\ngit reset --soft &lt;commit&gt;~1\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#add-and-commit-on-one-line","title":"Add and Commit on One Line","text":"<pre><code>git commit -am \"Git Add and Commit Shortcut\"\n#or\ngit commit -all -m \"Git Add and Commit Shortcut\"\n</code></pre> <p>This is equivalent to</p> <pre><code>git add .\ngit commit -m \"Your commit message\"\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#fix-previous-commit","title":"Fix Previous Commit","text":"<p>If you made a commit but realized you made a mistake, you can ammed the commit with the following.</p> <pre><code>git commit --ammend -m \"New message\"\n</code></pre>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/","title":"Infrastructure as Code","text":""},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#utilities","title":"Utilities","text":""},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#ansible","title":"Ansible","text":"<p>Uses playbooks written in YAML to automate Linux servers. One machine is a control node which sends the other managed nodes an Ansible module over SSH. Examples include updating network settings, provisioning databases, or any job you do more than once.</p> <p>The machine acting as the control node runs the CLI tools such as <code>ansible-playbook [file].yml</code> which runs on all managed nodes, sometimes referred to as \"hosts\", which are your target devices (severs, network appliances, VMs, etc.). Ansible does not need to be installed on the managed nodes, only the control node. Hosts are defined in an inventory file.</p> <p> The industry standard Ansible certification is the RHCE (Red Hat Certified Engineer). A prerequisite to this is the RHCSA (Red Hat Certified Systems Administrator), which is more comparable to the Linux+, except more geared towards RHEL distributions.</p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#terraform","title":"Terraform","text":"<p>Blueprints that automate everything you do in the cloud. Scripts are<code>.tf</code> files written in HashiCorp Configuration Language (HCL).</p> <p>People commonly use both Ansible and Terraform together, with Terraform for provisioning, and Ansible for configuring. Though many tasks can also be completed on boot with cloud-init (such as SSH and key setup).</p> <p></p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#saltstack","title":"SaltStack","text":"<p>Salt is a configuration management and orchestration tool written in Python currently currently supported by VMware after it's acquisition. It came from the need for communication and task execution to many systems in complex data centers. It gives administrators the ability to manage massive infrastructure at scale using the Salt remote execution engine, which stores configuration (state) data accessed through YAML.</p> <p>Salt is known for it's steep learning curve. </p> <p> Terraform was released in 2014 by Hashicorp under the permissive Mozilla Public License v2.0 (MPL 2.0) but recently changed the licensing to a Business Source License (BSL), which allows copying, modifying and redistribution, but restricts commercial use. This upset the community as the Open Source Initiative (OSI) definition of open source software does not discriminate against commercial venture. Rather than open source, it is more \"source-available\". As a response, the Linux Foundation created a fork of Terraform known as OpenTofu as an open source alternative.</p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#puppet-and-chef","title":"Puppet and Chef","text":"<p>Two less common automation tools written in Ruby.</p> <p>As a rule of thumb, whichever project has the cooler logo and has more Github star deserves more attention. In this case, Ansible is the clear winner.</p> <p></p>"},{"location":"3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#popularity-and-employment-compared","title":"Popularity and Employment Compared","text":"<p>Here are all the Infrastructure-as-Code tools popularity based on Github Stars. Ansible (pink) is by far the most popular followed by Terraform (green).</p> <p> Here are the number of jobs listed from Glassdoor in the United States in the month of November, 2023 mentioning these tools. </p>"}]}